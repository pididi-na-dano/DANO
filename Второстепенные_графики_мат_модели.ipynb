{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70165cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sdel=pd.read_excel(\"–°–¥–µ–ª–∫–∏_2025-11-25.xlsx\")\n",
    "proj=pd.read_excel(\"–ü—Ä–æ–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ_2025-11-25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === –ù–ê–°–¢–†–û–ô–ö–ò ===\n",
    "IQR_THRESHOLD = 1.5\n",
    "MIN_PRICE_UNIT = 1_000_000\n",
    "\n",
    "# === 0. –ü–û–î–ì–û–¢–û–í–ö–ê –ò –ë–ï–ó–û–ü–ê–°–ù–û–ï –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï (–ß–¢–û–ë–´ –ù–ê–ô–¢–ò –î–ê–¢–£ –°–¢–ê–†–¢–ê) ===\n",
    "sdel_clean = sdel.copy()\n",
    "proj_clean = proj.copy()\n",
    "\n",
    "# –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ —Å –¥–∞—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω—É–∂–Ω—ã\n",
    "col_dogovor = '–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)'\n",
    "col_reg = '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ (–º–µ—Å—è—Ü.–≥–æ–¥)'\n",
    "col_start = '–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞' # <--- –ü–†–û–í–ï–†–¨, –ß–¢–û –í –¢–ê–ë–õ–ò–¶–ï PROJ –û–ù–ê –ù–ê–ó–´–í–ê–ï–¢–°–Ø –¢–ê–ö –ñ–ï!\n",
    "\n",
    "# –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É —Å—Ç–∞—Ä—Ç–∞. –ï—Å–ª–∏ –µ—ë –Ω–µ—Ç –≤ —Å–¥–µ–ª–∫–∞—Ö, —Ç–∞—â–∏–º –∏–∑ –ø—Ä–æ–µ–∫—Ç–æ–≤\n",
    "if col_start not in sdel_clean.columns:\n",
    "    if col_start in proj_clean.columns:\n",
    "        # –ò—â–µ–º –∫–ª—é—á –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ —ç—Ç–æ ID –ø—Ä–æ–µ–∫—Ç–∞)\n",
    "        # –ü—Ä–æ–±—É–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: 'ID –ø—Ä–æ–µ–∫—Ç–∞', 'id_project', 'ID_PROJ'\n",
    "        merge_keys = [k for k in ['ID –ø—Ä–æ–µ–∫—Ç–∞', 'ID_–ø—Ä–æ–µ–∫—Ç–∞', 'id_project'] if k in sdel_clean.columns and k in proj_clean.columns]\n",
    "        \n",
    "        if merge_keys:\n",
    "            print(f\"‚úÖ –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º '{col_start}' –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ –∫–ª—é—á—É: {merge_keys[0]}...\")\n",
    "            sdel_clean = sdel_clean.merge(proj_clean[[merge_keys[0], col_start]], on=merge_keys[0], how='left')\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –Ω–∞—à–µ–ª –æ–±—â–∏–π ID –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü. '{col_start}' –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–æ–Ω–∫–∏ '{col_start}' –Ω–µ—Ç –Ω–∏ –≤ —Å–¥–µ–ª–∫–∞—Ö, –Ω–∏ –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö. –ü—Ä–æ–≤–µ—Ä—å –Ω–∞–∑–≤–∞–Ω–∏–µ!\")\n",
    "\n",
    "# === 1. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê (ETL) ===\n",
    "def clean_numeric_col(df, col_name):\n",
    "    if col_name in df.columns:\n",
    "        val = df[col_name].astype(str)\n",
    "        val = val.str.replace(\"\\u00a0\", \"\").str.replace(\" \", \"\").str.replace(\",\", \".\")\n",
    "        return pd.to_numeric(val, errors=\"coerce\").fillna(0)\n",
    "    return df[col_name] if col_name in df.columns else 0\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ —á–∏—Å–µ–ª\n",
    "target_cols = ['–°—É–º–º–∞ –±—é–¥–∂–µ—Ç–∞', '–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫', '–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫', \n",
    "               '–≠—Ç–∞–∂ –ª–æ—Ç–∞', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç']\n",
    "for col in target_cols:\n",
    "    sdel_clean[col] = clean_numeric_col(sdel_clean, col)\n",
    "\n",
    "# === –û–ß–ò–°–¢–ö–ê –î–ê–¢ (–°–ê–ú–û–ï –í–ê–ñ–ù–û–ï) ===\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ\n",
    "all_date_cols = [c for c in [col_dogovor, col_reg, col_start] if c in sdel_clean.columns]\n",
    "\n",
    "print(\"\\n=== –û–¢–ß–ï–¢ –ü–û –î–ê–¢–ê–ú (–î–û –§–ò–õ–¨–¢–†–ê–¶–ò–ò) ===\")\n",
    "for col in all_date_cols:\n",
    "    # 1. –ó–∞–ø–æ–º–∏–Ω–∞–µ–º —Å–∫–æ–ª—å–∫–æ –±—ã–ª–æ –ø—É—Å—Ç—ã—Ö –¥–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏\n",
    "    na_before = sdel_clean[col].isna().sum()\n",
    "    \n",
    "    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å dayfirst=True (–†–æ—Å—Å–∏–π—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –î–î.–ú–ú.–ì–ì–ì–ì)\n",
    "    # errors='coerce' –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç –º—É—Å–æ—Ä –≤ NaT, –Ω–æ –º—ã —ç—Ç–æ –æ—Ç—Å–ª–µ–¥–∏–º\n",
    "    sdel_clean[col] = pd.to_datetime(sdel_clean[col], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    # 3. –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å—Ç–∞–ª–æ –ø—É—Å—Ç—ã—Ö\n",
    "    na_after = sdel_clean[col].isna().sum()\n",
    "    lost = na_after - na_before\n",
    "    \n",
    "    print(f\"–ö–æ–ª–æ–Ω–∫–∞ '{col}':\")\n",
    "    if lost > 0:\n",
    "        print(f\"  ‚ùå –ë–ò–¢–´–ô –§–û–†–ú–ê–¢: {lost} –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å (–ø—Ä–µ–≤—Ä–∞—Ç–∏–ª–∏—Å—å –≤ NaT).\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã.\")\n",
    "\n",
    "# –†–∞—Å—á–µ—Ç —É–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫\n",
    "sdel_clean['cnt_safe'] = sdel_clean['–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫'].replace(0, 1)\n",
    "sdel_clean['Unit_Area'] = sdel_clean['–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫'] / sdel_clean['cnt_safe']\n",
    "sdel_clean['Unit_Price'] = sdel_clean['–°—É–º–º–∞ –±—é–¥–∂–µ—Ç–∞'] / sdel_clean['cnt_safe']\n",
    "sdel_clean['Price_m2'] = sdel_clean['–°—É–º–º–∞ –±—é–¥–∂–µ—Ç–∞'] / sdel_clean['–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "# === 2. –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –ü–õ–û–©–ê–î–¨ vs –ö–û–ú–ù–ê–¢–´ ===\n",
    "mask_area_rooms_outlier = pd.Series(False, index=sdel_clean.index)\n",
    "unique_rooms = sorted(sdel_clean['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç'].unique())\n",
    "\n",
    "print(\"\\n=== –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–õ–û–©–ê–î–ï–ô ===\")\n",
    "for room_cnt in unique_rooms:\n",
    "    idx_room = sdel_clean[sdel_clean['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç'] == room_cnt].index\n",
    "    if len(idx_room) == 0: continue\n",
    "    \n",
    "    subset_areas = sdel_clean.loc[idx_room, 'Unit_Area']\n",
    "    Q1 = subset_areas.quantile(0.25)\n",
    "    Q3 = subset_areas.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = max(Q1 - (IQR_THRESHOLD * IQR), 10.0)\n",
    "    upper = Q3 + (IQR_THRESHOLD * IQR)\n",
    "    \n",
    "    bad_indices = subset_areas[(subset_areas < lower) | (subset_areas > upper)].index\n",
    "    mask_area_rooms_outlier.loc[bad_indices] = True\n",
    "\n",
    "# === 3. –û–°–¢–ê–õ–¨–ù–´–ï –§–ò–õ–¨–¢–†–´ ===\n",
    "Q1_p = sdel_clean['Price_m2'].quantile(0.25)\n",
    "Q3_p = sdel_clean['Price_m2'].quantile(0.75)\n",
    "IQR_p = Q3_p - Q1_p\n",
    "mask_price_outlier = (sdel_clean['Price_m2'] < (Q1_p - 1.5*IQR_p)) | (sdel_clean['Price_m2'] > (Q3_p + 1.5*IQR_p))\n",
    "\n",
    "mask_cheap = sdel_clean['Unit_Price'] < MIN_PRICE_UNIT\n",
    "\n",
    "# === 4. –°–ë–û–†–ö–ê –ò–¢–û–ì–û–í–û–ì–û –î–ê–¢–ê–°–ï–¢–ê ===\n",
    "total_mask = mask_area_rooms_outlier | mask_price_outlier | mask_cheap \n",
    "sdel_final = sdel_clean[~total_mask].copy()\n",
    "\n",
    "# === 5. –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"–ò–¢–û–ì–ò –û–ß–ò–°–¢–ö–ò (–ë—ã–ª–æ: {len(sdel_clean)} -> –°—Ç–∞–ª–æ: {len(sdel_final)})\")\n",
    "print(\"-\" * 60)\n",
    "    \n",
    "print(\"-\" * 60)\n",
    "print(f\"–£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ –≤—Å–µ–≥–æ: {total_mask.sum()}\")\n",
    "sdel_final.drop(columns=['cnt_safe'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01298ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def process_real_estate_data(\n",
    "    proj: str, \n",
    "    deals: str, \n",
    "    bank_percentile_range: tuple = (0, 100),\n",
    "    bank_metric_for_filtering: str = 'sq_meters', # 'count', 'sq_meters', 'money'\n",
    "    weight_menric:str = 'sq_meters'\n",
    "):\n",
    "    \"\"\"\n",
    "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ETL –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö.\n",
    "    \n",
    "    –ú–û–î–ò–§–ò–ö–ê–¶–ò–Ø: \n",
    "    –†–∞—Å—á–µ—Ç Sellout (Sold_Area) —Ç–µ–ø–µ—Ä—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –°–¢–†–û–ì–û –ü–û –ò–ü–û–¢–ï–ß–ù–´–ú –°–î–ï–õ–ö–ê–ú,\n",
    "    —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –ª–æ–≥–∏–∫–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–ø–æ—Ç–µ—á–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è (–∫–∞–∫ –≤ Script A).\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    path_proj : str - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø—Ä–æ–µ–∫—Ç–æ–≤\n",
    "    path_deals : str - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–¥–µ–ª–æ–∫\n",
    "    bank_percentile_range : tuple - (min_p, max_p). –ü—Ä–∏–º–µ—Ä (25, 75).\n",
    "    bank_metric_for_filtering : str - –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –±–∞–Ω–∫–æ–≤.\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    ----------\n",
    "    df_ml : pd.DataFrame - –∏—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    bank_stats : pd.DataFrame - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∞–Ω–∫–∞–º\n",
    "    report : dict - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üöÄ –ó–ê–ü–£–°–ö –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–• (–†–ï–ñ–ò–ú: –ò–ü–û–¢–ï–ß–ù–´–ô SELLOUT)\")\n",
    "    print(f\"   –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–∞–Ω–∫–æ–≤-—Ñ–∏—á–µ–π: –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ {bank_percentile_range}, –º–µ—Ç—Ä–∏–∫–∞ '{bank_metric_for_filtering}'\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. –ó–ê–ì–†–£–ó–ö–ê –ò –û–ß–ò–°–¢–ö–ê –ß–ò–°–ï–õ\n",
    "    # ==========================================\n",
    "    def clean_numeric_col(df, col_name):\n",
    "        if col_name in df.columns:\n",
    "            val = df[col_name].astype(str)\n",
    "            val = val.str.replace(\"\\u00a0\", \"\").str.replace(\" \", \"\").str.replace(\",\", \".\")\n",
    "            return pd.to_numeric(val, errors=\"coerce\").fillna(0)\n",
    "        return df[col_name] if col_name in df.columns else 0\n",
    "\n",
    "    # –û—á–∏—Å—Ç–∫–∞ –ø–ª–æ—â–∞–¥–µ–π\n",
    "    for col in [\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\", \"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\", \n",
    "                \"–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫\", \"–°—É–º–º–∞—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Å–¥–µ–ª–æ–∫\"]:\n",
    "        if col in proj.columns: proj[col] = clean_numeric_col(proj, col)\n",
    "        if col in deals.columns: deals[col] = clean_numeric_col(deals, col)\n",
    "    \n",
    "    if \"–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫\" not in deals.columns:\n",
    "        deals[\"–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫\"] = 1\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –¥–∞—Ç\n",
    "    deals[\"dt_deal\"] = pd.to_datetime(deals[\"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)\"], dayfirst=True, errors=\"coerce\")\n",
    "    mask_date_na = deals[\"dt_deal\"].isna()\n",
    "    if mask_date_na.any():\n",
    "        deals.loc[mask_date_na, \"dt_deal\"] = pd.to_datetime(\n",
    "            \"01.\" + deals.loc[mask_date_na, \"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)\"].astype(str), \n",
    "            dayfirst=True, errors=\"coerce\"\n",
    "        )\n",
    "    deals = deals.dropna(subset=[\"dt_deal\"])\n",
    "\n",
    "    # –û—á–∏—Å—Ç–∫–∞ ID –∏ –ü—Ä–æ–µ–∫—Ç–æ–≤\n",
    "    for df in [proj, deals]:\n",
    "        df['ID –∫–æ—Ä–ø—É—Å–∞'] = df['ID –∫–æ—Ä–ø—É—Å–∞'].astype(str).str.replace(r'\\.0$', '', regex=True).str.strip()\n",
    "        if \"–ü—Ä–æ–µ–∫—Ç\" in df.columns: df[\"–ü—Ä–æ–µ–∫—Ç\"] = df[\"–ü—Ä–æ–µ–∫—Ç\"].astype(str).str.strip()\n",
    "\n",
    "    # –ú—ç–ø–ø–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤\n",
    "    corpus_map = proj[[\"ID –∫–æ—Ä–ø—É—Å–∞\", \"–ü—Ä–æ–µ–∫—Ç\", \"–ö–ª–∞—Å—Å –ø—Ä–æ–µ–∫—Ç–∞\"]].drop_duplicates().set_index(\"ID –∫–æ—Ä–ø—É—Å–∞\")\n",
    "    deals = deals.merge(corpus_map, on=\"ID –∫–æ—Ä–ø—É—Å–∞\", how=\"left\")\n",
    "    if \"–ü—Ä–æ–µ–∫—Ç_y\" in deals.columns:\n",
    "        deals[\"–ü—Ä–æ–µ–∫—Ç\"] = deals[\"–ü—Ä–æ–µ–∫—Ç_y\"].fillna(deals[\"–ü—Ä–æ–µ–∫—Ç_x\"])\n",
    "    deals = deals.dropna(subset=[\"–ü—Ä–æ–µ–∫—Ç\"])\n",
    "    \n",
    "    # –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π, –æ–±–æ–∑–Ω–∞—á–∞—é—â–∏—Ö –∏–ø–æ—Ç–µ–∫—É (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)\n",
    "    mortgage_flags = ['–¥–∞', 'yes', 'true', '1', '–∏–ø–æ—Ç–µ–∫–∞']\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. –ê–ù–ê–õ–ò–ó –ë–ê–ù–ö–û–í –ò –†–ê–°–ß–ï–¢ –í–ï–°–û–í\n",
    "    # ==========================================\n",
    "    print(\"\\nüè¶ –ê–ù–ê–õ–ò–ó –ë–ê–ù–ö–û–í–°–ö–û–ì–û –°–ï–ö–¢–û–†–ê...\")\n",
    "    \n",
    "    if \"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\" not in deals.columns:\n",
    "        deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"] = \"–ù–µ —É–∫–∞–∑–∞–Ω\"\n",
    "    deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"] = deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].fillna(\"–†–∞—Å—Å—Ä–æ—á–∫–∞/–ö—ç—à\").astype(str).str.strip()\n",
    "    \n",
    "    # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–Ω–∫–æ–≤ –±–µ—Ä–µ–º –í–°–ï —Å–¥–µ–ª–∫–∏ (—á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –æ–±—â–∏–π –æ–±—ä–µ–º —Ä—ã–Ω–∫–∞)\n",
    "    # –ù–æ –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ –ò–ü–û–¢–ï–ß–ù–´–• –∏–≥—Ä–æ–∫–æ–≤, –º–æ–∂–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å deals –ø–µ—Ä–µ–¥ groupby.\n",
    "    # –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –ª–æ–≥–∏–∫—É Script B: —Å–º–æ—Ç—Ä–∏–º –≤—Å–µ —Å–¥–µ–ª–∫–∏, –≥–¥–µ —É–∫–∞–∑–∞–Ω –±–∞–Ω–∫.\n",
    "    \n",
    "    grp_cols = {\"–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫\": \"sum\", \"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\": \"sum\"}\n",
    "    if \"–°—É–º–º–∞ –±—é–¥–∂–µ—Ç–∞\" in deals.columns:\n",
    "        grp_cols[\"–°—É–º–º–∞ –±—é–¥–∂–µ—Ç–∞\"] = \"sum\"\n",
    "        \n",
    "    bank_stats = deals.groupby(\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\").agg(grp_cols).reset_index()\n",
    "    \n",
    "    rename_map = {\n",
    "        \"–°—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫\": \"count\",\n",
    "        \"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\": \"sq_meters\",\n",
    "        \"–°—É–º–º–∞ –±—é–¥–∂–µ—Ç–∞\": \"money\"\n",
    "    }\n",
    "    bank_stats = bank_stats.rename(columns=rename_map)\n",
    "    if \"money\" not in bank_stats.columns: bank_stats[\"money\"] = 0\n",
    "    \n",
    "    total_sq = bank_stats[\"sq_meters\"].sum()\n",
    "    bank_stats[\"share_sq_meters\"] = (bank_stats[\"sq_meters\"] / total_sq * 100)\n",
    "    \n",
    "    target_metric = bank_metric_for_filtering if bank_metric_for_filtering in bank_stats.columns else 'sq_meters'\n",
    "    print(bank_stats.columns)\n",
    "    \n",
    "    threshold_low = np.percentile(bank_stats[target_metric], bank_percentile_range[0])\n",
    "    threshold_high = np.percentile(bank_stats[target_metric], bank_percentile_range[1])\n",
    "    \n",
    "    bank_stats[\"is_selected\"] = (bank_stats[target_metric] >= threshold_low) & \\\n",
    "                                (bank_stats[target_metric] <= threshold_high)\n",
    "    \n",
    "    bank_stats[\"bank_weight_score\"] = np.log1p(bank_stats[weight_menric])\n",
    "    \n",
    "    selected_banks = bank_stats[bank_stats[\"is_selected\"]][\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].tolist()\n",
    "    bank_weights_dict = bank_stats.set_index(\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\")[\"bank_weight_score\"].to_dict()\n",
    "    \n",
    "    print(f\"   –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤: {len(bank_stats)}\")\n",
    "    print(f\"   –û—Ç–æ–±—Ä–∞–Ω–æ –±–∞–Ω–∫–æ–≤ (–¥–∏–∞–ø–∞–∑–æ–Ω {bank_percentile_range}%): {len(selected_banks)}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. –ú–ê–ö–†–û-–î–ê–ù–ù–´–ï\n",
    "    # ==========================================\n",
    "    key_rate_data = [\n",
    "        ('2013-09-13', '2014-03-02', 5.50), ('2014-03-03', '2014-04-24', 7.00), ('2014-04-25', '2014-07-27', 7.50),\n",
    "        ('2014-07-28', '2014-11-04', 8.00), ('2014-11-05', '2014-12-11', 9.50), ('2014-12-12', '2014-12-15', 10.50),\n",
    "        ('2014-12-16', '2015-02-01', 17.00), ('2015-02-02', '2015-03-15', 15.00), ('2015-03-16', '2015-05-04', 14.00),\n",
    "        ('2015-05-05', '2015-06-15', 12.50), ('2015-06-16', '2015-08-02', 11.50), ('2015-08-03', '2016-06-13', 11.00),\n",
    "        ('2016-06-14', '2016-09-18', 10.50), ('2016-09-19', '2017-03-26', 10.00), ('2017-03-27', '2017-05-01', 9.75),\n",
    "        ('2017-05-02', '2017-06-18', 9.25), ('2017-06-19', '2017-09-17', 9.00), ('2017-09-18', '2017-10-29', 8.50),\n",
    "        ('2017-10-30', '2017-12-17', 8.25), ('2017-12-18', '2018-02-11', 7.75), ('2018-02-12', '2018-03-25', 7.50),\n",
    "        ('2018-03-26', '2018-09-16', 7.25), ('2018-09-17', '2018-12-16', 7.50), ('2018-12-17', '2019-06-16', 7.75),\n",
    "        ('2019-06-17', '2019-07-28', 7.50), ('2019-07-29', '2019-09-08', 7.25), ('2019-09-09', '2019-10-27', 7.00),\n",
    "        ('2019-10-28', '2019-12-15', 6.50), ('2019-12-16', '2020-02-09', 6.25), ('2020-02-10', '2020-04-26', 6.00),\n",
    "        ('2020-04-27', '2020-06-21', 5.50), ('2020-06-22', '2020-07-26', 4.50), ('2020-07-27', '2021-03-21', 4.25),\n",
    "        ('2021-03-22', '2021-04-25', 4.50), ('2021-04-26', '2021-06-14', 5.00), ('2021-06-15', '2021-07-25', 5.50),\n",
    "        ('2021-07-26', '2021-09-12', 6.50), ('2021-09-13', '2021-10-24', 6.75), ('2021-10-25', '2021-12-19', 7.50),\n",
    "        ('2021-12-20', '2022-02-13', 8.50), ('2022-02-14', '2022-02-27', 9.50), ('2022-02-28', '2022-04-10', 20.00),\n",
    "        ('2022-04-11', '2022-05-03', 17.00), ('2022-05-04', '2022-05-26', 14.00), ('2022-05-27', '2022-06-13', 11.00),\n",
    "        ('2022-06-14', '2022-07-24', 9.50), ('2022-07-25', '2022-09-18', 8.00), ('2022-09-19', '2022-12-31', 7.50),\n",
    "        ('2023-01-01', '2023-07-26', 7.5), ('2023-07-27', '2023-08-14', 8.5), ('2023-08-15', '2023-09-17', 12.0),\n",
    "        ('2023-09-18', '2023-10-29', 13.0), ('2023-10-30', '2023-12-17', 15.0), ('2023-12-18', '2024-07-28', 16.0),\n",
    "        ('2024-07-29', '2024-09-15', 18.0), ('2024-09-16', '2024-12-27', 19.0), ('2024-12-28', '2025-06-08', 21.0)\n",
    "    ]\n",
    "    macro_range = pd.date_range(start='2013-09-13', end='2026-01-01', freq='D')\n",
    "    macro_df = pd.DataFrame(index=macro_range)\n",
    "    macro_df['key_rate'] = np.nan\n",
    "    macro_df['is_subsidy'] = 0\n",
    "\n",
    "    for start, end, rate in key_rate_data:\n",
    "        mask = (macro_df.index >= pd.to_datetime(start)) & (macro_df.index <= pd.to_datetime(end))\n",
    "        macro_df.loc[mask, 'key_rate'] = rate\n",
    "\n",
    "    macro_df['key_rate'] = macro_df['key_rate'].ffill()\n",
    "    macro_df.loc[(macro_df.index >= '2020-04-17') & (macro_df.index < '2024-07-01'), 'is_subsidy'] = 1\n",
    "    macro_monthly = macro_df['key_rate'].resample('MS').mean()\n",
    "\n",
    "    def get_macro_features(start_date, end_date):\n",
    "        subset_daily = macro_df[(macro_df.index >= start_date) & (macro_df.index <= end_date)]\n",
    "        if subset_daily.empty: return np.nan, np.nan, 0, np.nan\n",
    "        kr_start = subset_daily['key_rate'].iloc[0]\n",
    "        kr_spread = subset_daily['key_rate'].max() - subset_daily['key_rate'].min()\n",
    "        sub_share = subset_daily['is_subsidy'].mean()\n",
    "        subset_monthly = macro_monthly[(macro_monthly.index >= start_date) & (macro_monthly.index <= end_date)]\n",
    "        kr_mean_monthly = subset_daily['key_rate'].mean() if subset_monthly.empty else subset_monthly.mean()\n",
    "        return kr_start, kr_spread, sub_share, kr_mean_monthly\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. –°–ë–û–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê (MAIN LOOP)\n",
    "    # ==========================================\n",
    "    corpus_starts = deals.groupby(\"ID –∫–æ—Ä–ø—É—Å–∞\")[\"dt_deal\"].min().reset_index().rename(columns={\"dt_deal\": \"corpus_start\"})\n",
    "    proj = proj.merge(corpus_starts, on=\"ID –∫–æ—Ä–ø—É—Å–∞\", how=\"left\")\n",
    "    \n",
    "    temp_proj_starts = proj.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[\"corpus_start\"].min().reset_index().rename(columns={\"corpus_start\": \"project_start_implied\"})\n",
    "    proj = proj.merge(temp_proj_starts, on=\"–ü—Ä–æ–µ–∫—Ç\", how=\"left\")\n",
    "    proj[\"corpus_start\"] = proj[\"corpus_start\"].fillna(proj[\"project_start_implied\"])\n",
    "    \n",
    "    proj_starts = proj.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[\"corpus_start\"].min().reset_index().rename(columns={\"corpus_start\": \"project_start\"})\n",
    "    MAX_DATE = deals[\"dt_deal\"].max()\n",
    "    \n",
    "    data_list = []\n",
    "    projects_list = proj_starts[\"–ü—Ä–æ–µ–∫—Ç\"].unique()\n",
    "    \n",
    "    stats_cnt = {\"total\": len(projects_list), \"success\": 0, \"skipped_young\": 0, \"skipped_no_sales\": 0, \"dropped_bad_so\": 0}\n",
    "\n",
    "    print(f\"\\nüîÑ –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–ï–ö–¢–û–í ({len(projects_list)} —à—Ç)...\")\n",
    "\n",
    "    for project in projects_list:\n",
    "        t0 = proj_starts.loc[proj_starts[\"–ü—Ä–æ–µ–∫—Ç\"] == project, \"project_start\"].values[0]\n",
    "        t0 = pd.to_datetime(t0)\n",
    "        if pd.isna(t0): continue\n",
    "        \n",
    "        p_class = proj[proj[\"–ü—Ä–æ–µ–∫—Ç\"] == project][\"–ö–ª–∞—Å—Å –ø—Ä–æ–µ–∫—Ç–∞\"].iloc[0]\n",
    "        \n",
    "        # --- –ì–û–î 1 ---\n",
    "        t_end_y1 = t0 + timedelta(days=365)\n",
    "        \n",
    "        if (MAX_DATE - t0).days < 365:\n",
    "            stats_cnt[\"skipped_young\"] += 1\n",
    "            continue\n",
    "\n",
    "        # –ü–õ–û–©–ê–î–¨ (–ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å)\n",
    "        valid_corp_y1 = proj[(proj[\"–ü—Ä–æ–µ–∫—Ç\"] == project) & (proj[\"corpus_start\"] <= t_end_y1)]\n",
    "        area_planned_y1 = valid_corp_y1[\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"].sum()\n",
    "        \n",
    "        # –°–î–ï–õ–ö–ò (–ß–∏—Å–ª–∏—Ç–µ–ª—å) - –¢–ï–ü–ï–†–¨ –°–ß–ò–¢–ê–ï–ú –¢–û–õ–¨–ö–û –ò–ü–û–¢–ï–ß–ù–´–ï –°–î–ï–õ–ö–ò\n",
    "        mask_sales_y1 = (deals[\"–ü—Ä–æ–µ–∫—Ç\"] == project) & (deals[\"dt_deal\"] >= t0) & (deals[\"dt_deal\"] <= t_end_y1)\n",
    "        deals_subset_y1 = deals[mask_sales_y1]\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä: –¢–æ–ª—å–∫–æ –∏–ø–æ—Ç–µ—á–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ Sellout\n",
    "        mortgage_deals_y1 = deals_subset_y1[deals_subset_y1[\"–ò–ø–æ—Ç–µ–∫–∞\"].astype(str).str.lower().isin(mortgage_flags)]\n",
    "        \n",
    "        # –í–ê–ñ–ù–û: sales_y1 —Ç–µ–ø–µ—Ä—å = –ø–ª–æ—â–∞–¥—å –∏–ø–æ—Ç–µ—á–Ω—ã—Ö —Å–¥–µ–ª–æ–∫ (–∫–∞–∫ –≤ Script A)\n",
    "        sales_y1 = mortgage_deals_y1[\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"].sum()\n",
    "        \n",
    "        # –ï—Å–ª–∏ –ø—Ä–æ–µ–∫—Ç –≤–æ–æ–±—â–µ –Ω–µ –ø—Ä–æ–¥–∞–≤–∞–ª—Å—è –≤ –∏–ø–æ—Ç–µ–∫—É –∏–ª–∏ –Ω–µ—Ç –ø–ª–æ—â–∞–¥–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∏–ª–∏ —Å—Ç–∞–≤–∏–º 0, –Ω–æ –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å)\n",
    "        if sales_y1 <= 0 or area_planned_y1 <= 0:\n",
    "            stats_cnt[\"skipped_no_sales\"] += 1\n",
    "            continue\n",
    "\n",
    "        # –ú–ê–ö–†–û\n",
    "        kr_start, kr_spread, sub_share, kr_mean = get_macro_features(t0, t_end_y1)\n",
    "        \n",
    "        # –î–û–ü. –ú–ï–¢–†–ò–ö–ò (–î–æ–ª—è –∏–ø–æ—Ç–µ–∫–∏ –æ—Ç –≤—Å–µ—Ö —Å–¥–µ–ª–æ–∫)\n",
    "        # –°—á–∏—Ç–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Å–µ–≥–æ –æ–±—ä–µ–º–∞ —Å–¥–µ–ª–æ–∫ (deals_subset_y1), –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∏–ø–æ—Ç–µ—á–Ω—ã—Ö\n",
    "        mort_share = len(mortgage_deals_y1) / len(deals_subset_y1) if len(deals_subset_y1) > 0 else 0\n",
    "        \n",
    "        # –§–ò–ß–ò –ë–ê–ù–ö–û–í (–°—á–∏—Ç–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–ø–æ—Ç–µ—á–Ω–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏)\n",
    "        banks_in_project = mortgage_deals_y1[mortgage_deals_y1[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].isin(selected_banks)][\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].unique()\n",
    "        \n",
    "        num_banks_filtered = len(banks_in_project)\n",
    "        bank_weighted_index = sum([bank_weights_dict.get(b, 0) for b in banks_in_project])\n",
    "\n",
    "        so_val = (sales_y1 / area_planned_y1) * 100\n",
    "        \n",
    "        data_list.append({\n",
    "            \"Project\": project,\n",
    "            \"Class\": p_class,\n",
    "            \"Year_Num\": 1,\n",
    "            \"Planned_Area\": area_planned_y1,\n",
    "            \"Sold_Area\": sales_y1, # –≠—Ç–æ Mortgage Sold Area\n",
    "            \"Sellout\": so_val,     # –≠—Ç–æ Mortgage Sellout\n",
    "            \"KR_Start\": kr_start,\n",
    "            \"KR_Spread\": kr_spread,\n",
    "            \"KR_Mean\": kr_mean,\n",
    "            \"Subsidy_Share\": sub_share,\n",
    "            \"Mortgage_Share\": mort_share,\n",
    "            \"Num_Banks_Filtered\": num_banks_filtered,\n",
    "            \"Bank_Index_Weighted\": bank_weighted_index,\n",
    "            \"Log_Area\": np.log1p(area_planned_y1)\n",
    "        })\n",
    "\n",
    "        # --- –ì–û–î 2 ---\n",
    "        t_start_y2 = t_end_y1 + timedelta(days=1)\n",
    "        t_end_y2 = t0 + timedelta(days=730)\n",
    "        \n",
    "        if (MAX_DATE - t0).days >= 730:\n",
    "            valid_corp_y2 = proj[(proj[\"–ü—Ä–æ–µ–∫—Ç\"] == project) & (proj[\"corpus_start\"] <= t_end_y2)]\n",
    "            area_planned_y2 = valid_corp_y2[\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"].sum()\n",
    "            \n",
    "            mask_sales_y2 = (deals[\"–ü—Ä–æ–µ–∫—Ç\"] == project) & (deals[\"dt_deal\"] >= t_start_y2) & (deals[\"dt_deal\"] <= t_end_y2)\n",
    "            deals_subset_y2 = deals[mask_sales_y2]\n",
    "            \n",
    "            # –§–∏–ª—å—Ç—Ä: –¢–æ–ª—å–∫–æ –∏–ø–æ—Ç–µ—á–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ Sellout (–ì–æ–¥ 2)\n",
    "            mortgage_deals_y2 = deals_subset_y2[deals_subset_y2[\"–ò–ø–æ—Ç–µ–∫–∞\"].astype(str).str.lower().isin(mortgage_flags)]\n",
    "            sales_y2 = mortgage_deals_y2[\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"].sum()\n",
    "            \n",
    "            kr_start_2, kr_spread_2, sub_share_2, kr_mean_2 = get_macro_features(t_start_y2, t_end_y2)\n",
    "            \n",
    "            mort_share_2 = len(mortgage_deals_y2) / len(deals_subset_y2) if len(deals_subset_y2) > 0 else 0\n",
    "            \n",
    "            banks_in_project_2 = mortgage_deals_y2[mortgage_deals_y2[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].isin(selected_banks)][\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].unique()\n",
    "            bank_weighted_index_2 = sum([bank_weights_dict.get(b, 0) for b in banks_in_project_2])\n",
    "\n",
    "            if area_planned_y2 > 0 and sales_y2 > 0:\n",
    "                data_list.append({\n",
    "                    \"Project\": project,\n",
    "                    \"Class\": p_class,\n",
    "                    \"Year_Num\": 2,\n",
    "                    \"Planned_Area\": area_planned_y2,\n",
    "                    \"Sold_Area\": sales_y2, # Mortgage Area\n",
    "                    \"Sellout\": (sales_y2 / area_planned_y2) * 100, # Mortgage Sellout\n",
    "                    \"KR_Start\": kr_start_2,\n",
    "                    \"KR_Spread\": kr_spread_2,\n",
    "                    \"KR_Mean\": kr_mean_2,\n",
    "                    \"Subsidy_Share\": sub_share_2,\n",
    "                    \"Mortgage_Share\": mort_share_2,\n",
    "                    \"Num_Banks_Filtered\": len(banks_in_project_2),\n",
    "                    \"Bank_Index_Weighted\": bank_weighted_index_2,\n",
    "                    \"Log_Area\": np.log1p(area_planned_y2)\n",
    "                })\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. –§–ò–ù–ê–õ–ò–ó–ê–¶–ò–Ø\n",
    "    # ==========================================\n",
    "    df_ml = pd.DataFrame(data_list)\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä –≤—ã–±—Ä–æ—Å–æ–≤ (>100%)\n",
    "    if not df_ml.empty:\n",
    "        n_before = len(df_ml)\n",
    "        df_ml = df_ml[df_ml[\"Sellout\"] <= 100]\n",
    "        stats_cnt[\"dropped_bad_so\"] = n_before - len(df_ml)\n",
    "        stats_cnt[\"success\"] = len(df_ml)\n",
    "    \n",
    "    print(\"\\n‚úÖ –ì–û–¢–û–í–û!\")\n",
    "    print(f\"   –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–æ–∫ –¥–ª—è ML: {len(df_ml)}\")\n",
    "    print(f\"   –û—à–∏–±–æ–∫ Sellout>100%: {stats_cnt['dropped_bad_so']}\")\n",
    "\n",
    "    report = stats_cnt\n",
    "    \n",
    "    return df_ml, bank_stats, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed707d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∏–ª—è\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 6) # –°–¥–µ–ª–∞–ª–∏ –ø–æ—à–∏—Ä–µ, —á—Ç–æ–±—ã –≤–º–µ—Å—Ç–∏—Ç—å 3 –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –≤ –æ—Ç—á–µ—Ç–∞—Ö\n",
    "TRANS_MAP = {\n",
    "    \"const\": \"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞\",\n",
    "    \"Num_Banks_Filtered\": \"–ö–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤\",\n",
    "    \"Bank_Index_Weighted\": \"–í–µ—Å –±–∞–Ω–∫–æ–≤ (Log –ø–ª–æ—â–∞–¥–∏)\",\n",
    "    \"Log_Area\": \"–ú–∞—Å—à—Ç–∞–± –ø—Ä–æ–µ–∫—Ç–∞ (Log –º¬≤)\",\n",
    "    \"KR_Mean\": \"–°—Ä. –ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞\",\n",
    "    \"Subsidy_Share\": \"–î–æ–ª—è –°—É–±—Å–∏–¥–∏–π\",\n",
    "    \"Mortgage_Share\": \"–î–æ–ª—è –ò–ø–æ—Ç–µ–∫–∏\",\n",
    "    \"Sellout\": \"–°–µ–ª–∞—É—Ç (%)\"\n",
    "}\n",
    "\n",
    "def advanced_analysis(df, features, target, title, year_filter=None):\n",
    "    \"\"\"\n",
    "    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –°—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å, –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç + 3 –≥—Ä–∞—Ñ–∏–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–≤–∫–ª. QQ-Plot).\n",
    "    \"\"\"\n",
    "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "    if year_filter:\n",
    "        data = df[df[\"Year_Num\"] == year_filter].copy()\n",
    "        prefix = f\"[–ì–û–î {year_filter}]\"\n",
    "    else:\n",
    "        data = df.copy()\n",
    "        prefix = \"[–í–°–ï –ì–û–î–´]\"\n",
    "        \n",
    "    full_title = f\"{prefix} {title}\"\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞\n",
    "    data = data.dropna(subset=features + [target])\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ X –∏ y\n",
    "    X = sm.add_constant(data[features])\n",
    "    y = data[target]\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # === 1. –¢–ï–ö–°–¢–û–í–û–ô –û–¢–ß–ï–¢ ===\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ –ú–û–î–ï–õ–¨: {full_title}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    print(f\"üìå R-squared (–¢–æ—á–Ω–æ—Å—Ç—å):      {model.rsquared:.4f}\")\n",
    "    print(f\"üìå Adj. R-squared (–ß–µ—Å—Ç–Ω—ã–π):  {model.rsquared_adj:.4f}\")\n",
    "    print(f\"üìå F-statistic p-value:       {model.f_pvalue:.4e}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # –ö—Ä–∞—Å–∏–≤–∞—è —Ç–∞–±–ª–∏—á–∫–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç\": model.params,\n",
    "        \"Std. Error\": model.bse,\n",
    "        \"P-–∑–Ω–∞—á–µ–Ω–∏–µ\": model.pvalues,\n",
    "        \"–ó–Ω–∞—á–∏–º–æ—Å—Ç—å\": model.pvalues.apply(lambda p: \"‚≠ê‚≠ê‚≠ê\" if p<0.001 else (\"‚≠ê‚≠ê\" if p<0.01 else (\"‚≠ê\" if p<0.05 else \"‚ùå\")))\n",
    "    })\n",
    "    \n",
    "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    summary_df.index = summary_df.index.map(lambda x: TRANS_MAP.get(x, x))\n",
    "    \n",
    "    # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã\n",
    "    print(summary_df)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # === 2. –ì–†–ê–§–ò–ö–ò (–¢–ï–ü–ï–†–¨ 3 –®–¢–£–ö–ò) ===\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    plt.suptitle(full_title, fontsize=14, weight='bold')\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 1: –†–µ–≥—Ä–µ—Å—Å–∏—è (–∏–ª–∏ Predicted vs Actual)\n",
    "    ax1 = axes[0]\n",
    "    if len(features) == 1:\n",
    "        sns.regplot(data=data, x=features[0], y=target, ax=ax1, \n",
    "                    scatter_kws={'alpha':0.5, 's': 30}, line_kws={'color': 'red'})\n",
    "        ax1.set_title(f\"–†–µ–≥—Ä–µ—Å—Å–∏—è: {TRANS_MAP.get(features[0], features[0])}\")\n",
    "    else:\n",
    "        preds = model.predict(X)\n",
    "        ax1.scatter(y, preds, alpha=0.5, edgecolors='b')\n",
    "        ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "        ax1.set_xlabel(\"–†–µ–∞–ª—å–Ω—ã–π —Ñ–∞–∫—Ç\")\n",
    "        ax1.set_ylabel(\"–ü—Ä–æ–≥–Ω–æ–∑ –º–æ–¥–µ–ª–∏\")\n",
    "        ax1.set_title(\"–§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑\")\n",
    "        \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 2: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
    "    ax2 = axes[1]\n",
    "    residuals = model.resid\n",
    "    sns.histplot(residuals, kde=True, ax=ax2, color='green')\n",
    "    ax2.set_title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ (–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞)\")\n",
    "    ax2.axvline(0, color='red', linestyle='--')\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 3: Q-Q Plot (–ù–û–í–´–ô)\n",
    "    ax3 = axes[2]\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Å—Ç–∞—Ç–∫–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ª–∏–Ω–∏–∏ 45 –≥—Ä–∞–¥—É—Å–æ–≤\n",
    "    sm.qqplot(residuals, line='45', fit=True, ax=ax3)\n",
    "    ax3.set_title(\"Q-Q Plot (–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–ö–ê–ñ–ò –°–í–û–ò –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú\n",
    "FILE_PROJ = \"–ü—Ä–æ–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ_2025-11-25.xlsx\" # <-- –ü—Ä–æ–≤–µ—Ä—å –∏–º—è —Ñ–∞–π–ª–∞\n",
    "FILE_DEALS = \"–°–¥–µ–ª–∫–∏_2025-11-25.xlsx\"           # <-- –ü—Ä–æ–≤–µ—Ä—å –∏–º—è —Ñ–∞–π–ª–∞\n",
    "\n",
    "print(\"‚è≥ –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê 1: –í–°–ï –ë–ê–ù–ö–ò (0-100%)...\")\n",
    "df_all, banks, stat = process_real_estate_data(\n",
    "    proj, sdel_final, \n",
    "    bank_percentile_range=(0, 100), \n",
    "    bank_metric_for_filtering='sq_meters'\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê 2: –¢–û–õ–¨–ö–û –ö–†–£–ü–ù–´–ï –ë–ê–ù–ö–ò (40-100%)...\")\n",
    "df_filtered, _, _ = process_real_estate_data(\n",
    "    proj, sdel_final, \n",
    "    bank_percentile_range=(40, 100), \n",
    "    bank_metric_for_filtering='sq_meters'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ –ì–û–¢–û–í–û. –°—Ç—Ä–æ–∫ –≤ df_all: {len(df_all)}, –°—Ç—Ä–æ–∫ –≤ df_filtered: {len(df_filtered)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff=pd.read_excel(\"–°–¥–µ–ª–∫–∏_2025-11-25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfff.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51684c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfff['–¢–µ–∫—É—â–∞—è —Å—Ç–∞–¥–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "banks.sort_values(by='sq_meters', ascending=False, inplace=True)\n",
    "print(\"\\nüè¶ –¢–û–ü-10 –ë–ê–ù–ö–û–í –ü–û –û–ë–™–ï–ú–£ –ü–†–û–î–ê–ñ\")\n",
    "banks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc168ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_all[df_all[\"Year_Num\"]==1]))\n",
    "print(len(df_all[df_all[\"Year_Num\"]==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ee9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_all, ['Num_Banks_Filtered'], 'Sellout', \"–ü—Ä–æ—Å—Ç–∞—è: –ö–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤ (–ë–ï–ó –§–ò–õ–¨–¢–†–ê)\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_all, ['Num_Banks_Filtered'], 'Sellout', \"–ü—Ä–æ—Å—Ç–∞—è: –ö–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤ (–ë–ï–ó –§–ò–õ–¨–¢–†–ê)\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e819d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_filtered, ['Num_Banks_Filtered'], 'Sellout', \"–ü—Ä–æ—Å—Ç–∞—è: –ö–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤ (–§–ò–õ–¨–¢–† 40-100%)\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_filtered, ['Num_Banks_Filtered'], 'Sellout', \"–ü—Ä–æ—Å—Ç–∞—è: –ö–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤ (–§–ò–õ–¨–¢–† 40-100%)\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_all, ['Bank_Index_Weighted'], 'Sellout', \"–ü—Ä–æ—Å—Ç–∞—è: –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –±–∞–Ω–∫–æ–≤\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_all, ['Bank_Index_Weighted'], 'Sellout', \"–ü—Ä–æ—Å—Ç–∞—è: –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –±–∞–Ω–∫–æ–≤\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2057e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–∫—Ä–æ–º–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–¥–µ–ª–æ–∫, –∫–∞–∫ —Ç—ã –ø—Ä–æ—Å–∏–ª)\n",
    "features_full = ['Num_Banks_Filtered', 'Log_Area', 'KR_Mean', 'Mortgage_Share']\n",
    "\n",
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_all, features_full, 'Sellout', \"–ú—É–ª—å—Ç–∏: –í—Å–µ —Ñ–∏—á–∏ (–í–°–ï –ë–ê–ù–ö–ò)\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_all, features_full, 'Sellout', \"–ú—É–ª—å—Ç–∏: –í—Å–µ —Ñ–∏—á–∏ (–í–°–ï –ë–ê–ù–ö–ò)\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_full = ['Num_Banks_Filtered', 'Log_Area', 'KR_Mean', 'Mortgage_Share']\n",
    "\n",
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_filtered, features_full, 'Sellout', \"–ú—É–ª—å—Ç–∏: –í—Å–µ —Ñ–∏—á–∏ (–§–ò–õ–¨–¢–† –ë–ê–ù–ö–û–í 40%)\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_filtered, features_full, 'Sellout', \"–ú—É–ª—å—Ç–∏: –í—Å–µ —Ñ–∏—á–∏ (–§–ò–õ–¨–¢–† –ë–ê–ù–ö–û–í 40%)\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87746198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–º–µ–Ω–∏–ª–∏ –∫–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤ –Ω–∞ –∏—Ö –í–µ—Å\n",
    "features_weighted = ['Bank_Index_Weighted', 'Log_Area', 'KR_Mean']\n",
    "\n",
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_all, features_weighted, 'Sellout', \"–ú—É–ª—å—Ç–∏: –í–ï–° –ë–ê–ù–ö–û–í –≤–º–µ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_all, features_weighted, 'Sellout', \"–ú—É–ª—å—Ç–∏: –í–ï–° –ë–ê–ù–ö–û–í –≤–º–µ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc00e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mizir, _, _ = process_real_estate_data(\n",
    "    proj, sdel_final, \n",
    "    bank_percentile_range=(40, 80), \n",
    "    bank_metric_for_filtering='sq_meters'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ –ì–û–¢–û–í–û. –°—Ç—Ä–æ–∫ –≤ df_all: {len(df_mizir)}, –°—Ç—Ä–æ–∫ –≤ df_filtered: {len(df_mizir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–º–µ–Ω–∏–ª–∏ –∫–æ–ª-–≤–æ –±–∞–Ω–∫–æ–≤ –Ω–∞ –∏—Ö –í–µ—Å\n",
    "features_weighted = ['Bank_Index_Weighted', 'Log_Area', 'KR_Mean',  'Mortgage_Share']\n",
    "\n",
    "# --- –ì–û–î 1 ---\n",
    "advanced_analysis(df_mizir, features_weighted, 'Sellout', \"–í–õ–ò–Ø–ù–ò–Ø –í–ï–°–ê –ë–ê–ù–ö–û–í –ë–ï–ó –¢–û–ü 20%\", year_filter=1)\n",
    "\n",
    "# --- –ì–û–î 2 ---\n",
    "advanced_analysis(df_mizir, features_weighted, 'Sellout', \"–í–õ–ò–Ø–ù–ò–Ø –í–ï–°–ê –ë–ê–ù–ö–û–í –ë–ï–ó –¢–û–ü 20%\", year_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "# ==============================================================================\n",
    "# ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò (–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø)\n",
    "# ==============================================================================\n",
    "PATH_DEALS_FILE = \"–°–¥–µ–ª–∫–∏_2025-11-25.xlsx\"  # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–¥–µ–ª–æ–∫\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –≤–µ—Å–æ–≤: 0.0 - —Å–∞–º–æ–µ –Ω–∞—á–∞–ª–æ, 1.0 - —Å–¥–∞–Ω\n",
    "# –ó–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∫–∞–ª–∏–±—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –≤–∞—à–∏ –æ—â—É—â–µ–Ω–∏—è —Ä—ã–Ω–∫–∞\n",
    "STAGE_WEIGHTS_MAP = {\n",
    "    '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–µ –Ω–∞—á–∞—Ç–æ': 0.0,\n",
    "    '–†–∞–±–æ—Ç—ã –Ω—É–ª–µ–≤–æ–≥–æ —Ü–∏–∫–ª–∞': 0.1,\n",
    "    '–ù–∞—á–∞–ª–æ –º–æ–Ω—Ç–∞–∂–Ω—ã—Ö —Ä–∞–±–æ—Ç': 0.25,\n",
    "    '–ú–æ–Ω—Ç–∞–∂–Ω—ã–µ –∏ –æ—Ç–¥–µ–ª–æ—á–Ω—ã–µ —Ä–∞–±–æ—Ç—ã': 0.6,\n",
    "    '–ü–æ–ª—É—á–µ–Ω–∏–µ –†–í–≠, –±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏': 0.9,\n",
    "    '–í–≤–µ–¥–µ–Ω –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é': 1.0\n",
    "}\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "# –í–ê–ñ–ù–û: Bank_Index_Weighted –∏ KR_Mean –±–µ—Ä—É—Ç—Å—è –∏–∑ df_all, \n",
    "# –∞ Construction_Stage_Mean –º—ã —Å–µ–π—á–∞—Å –ø–æ—Å—á–∏—Ç–∞–µ–º.\n",
    "MODEL_FEATURES = [\n",
    "    'Bank_Index_Weighted',      # –í–µ—Å –±–∞–Ω–∫–æ–≤ (–∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏)\n",
    "    'Construction_Stage_Mean',  # –°—Ä–µ–¥–Ω—è—è —Å—Ç–∞–¥–∏—è —Å—Ç—Ä–æ–π–∫–∏ (–ù–æ–≤–∞—è —Ñ–∏—á–∞)\n",
    "    'KR_Mean'                   # –ú–∞–∫—Ä–æ (–°—Ç–∞–≤–∫–∞ –¶–ë)\n",
    "]\n",
    "TARGET_VARIABLE = 'Sellout'\n",
    "\n",
    "# ==============================================================================\n",
    "# üõ† –≠–¢–ê–ü 1: ETL (–†–ê–°–ß–ï–¢ –°–¢–ê–î–ò–ò –°–¢–†–û–ò–¢–ï–õ–¨–°–¢–í–ê –ó–ê 1 –ì–û–î)\n",
    "# ==============================================================================\n",
    "print(f\"üöÄ –ó–ê–ü–£–°–ö: –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ '–°—Ä–µ–¥–Ω—è—è —Å—Ç–∞–¥–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞' (–∑–∞ 1-–π –≥–æ–¥ –ø—Ä–æ–¥–∞–∂)...\")\n",
    "\n",
    "# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è df_all\n",
    "if 'df_all' not in locals():\n",
    "    raise ValueError(\"‚ö†Ô∏è –û–®–ò–ë–ö–ê: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è df_all –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (process_real_estate_data).\")\n",
    "\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–¥–µ–ª–æ–∫\n",
    "deals_raw = pd.read_excel(PATH_DEALS_FILE)\n",
    "\n",
    "# 3. –û—á–∏—Å—Ç–∫–∞ –¥–∞—Ç (–∫–∞–∫ –≤ process_real_estate_data)\n",
    "date_col = \"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)\"\n",
    "deals_raw[\"dt_deal\"] = pd.to_datetime(deals_raw[date_col], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# –ï—Å–ª–∏ –µ—Å—Ç—å –±–∏—Ç—ã–µ –¥–∞—Ç—ã, –ø—Ä–æ–±—É–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ \"01.\"\n",
    "mask_na = deals_raw[\"dt_deal\"].isna()\n",
    "if mask_na.any():\n",
    "    deals_raw.loc[mask_na, \"dt_deal\"] = pd.to_datetime(\n",
    "        \"01.\" + deals_raw.loc[mask_na, date_col].astype(str), \n",
    "        dayfirst=True, errors=\"coerce\"\n",
    "    )\n",
    "deals_raw = deals_raw.dropna(subset=[\"dt_deal\", \"–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö\"])\n",
    "\n",
    "# 4. –ú—ç–ø–ø–∏–Ω–≥ —Å—Ç–∞–¥–∏–π\n",
    "# –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É. –ï—Å–ª–∏ —Å—Ç–∞–¥–∏–∏ –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ -> fillna(0)\n",
    "deals_raw['Stage_Score'] = deals_raw['–¢–µ–∫—É—â–∞—è —Å—Ç–∞–¥–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞'].map(STAGE_WEIGHTS_MAP).fillna(0)\n",
    "\n",
    "# 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–∫–Ω–∞ \"1 –≥–æ–¥ –ø—Ä–æ–¥–∞–∂\" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "# –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—É—é —Ä–∞–Ω–Ω—é—é –¥–∞—Ç—É —Å–¥–µ–ª–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–æ–µ–∫—Ç—É\n",
    "proj_start_dates = deals_raw.groupby(\"–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö\")[\"dt_deal\"].min().reset_index().rename(columns={\"dt_deal\": \"Project_Start_Date\"})\n",
    "deals_raw = deals_raw.merge(proj_start_dates, on=\"–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö\", how=\"left\")\n",
    "\n",
    "# –§–∏–ª—å—Ç—Ä—É–µ–º: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–¥–µ–ª–∫–∏, —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –≤ –ø–µ—Ä–≤—ã–µ 365 –¥–Ω–µ–π\n",
    "deals_y1 = deals_raw[\n",
    "    (deals_raw[\"dt_deal\"] >= deals_raw[\"Project_Start_Date\"]) & \n",
    "    (deals_raw[\"dt_deal\"] <= deals_raw[\"Project_Start_Date\"] + timedelta(days=365))\n",
    "].copy()\n",
    "\n",
    "# 6. –ê–≥—Ä–µ–≥–∞—Ü–∏—è: –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ Stage_Score\n",
    "# –¢–∞–∫ –∫–∞–∫ –º—ã —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Å–¥–µ–ª–æ–∫, —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ \"–≤–∑–≤–µ—à–∏–≤–∞–µ—Ç\" —Å—Ç–∞–¥–∏—é –ø–æ –æ–±—ä–µ–º—É –ø—Ä–æ–¥–∞–∂ (–≤ —à—Ç—É–∫–∞—Ö)\n",
    "stage_stats = deals_y1.groupby(\"–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö\")[\"Stage_Score\"].mean().reset_index()\n",
    "stage_stats = stage_stats.rename(columns={\"Stage_Score\": \"Construction_Stage_Mean\", \"–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö\": \"Project\"})\n",
    "\n",
    "print(f\"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(stage_stats)}\")\n",
    "print(f\"   –ü—Ä–∏–º–µ—Ä—ã —Å—Ä–µ–¥–Ω–∏—Ö —Å—Ç–∞–¥–∏–π (–¢–æ–ø-3):\")\n",
    "print(stage_stats.head(3).to_string(index=False))\n",
    "print(f\"—Å—Ä–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –ø—Ä–æ–µ–∫—Ç–∞–º: {stage_stats['Construction_Stage_Mean'].mean():.4f}\")\n",
    "print(f\"–º–∏–Ω–∏–º—É–º: {stage_stats['Construction_Stage_Mean'].min():.4f}, –º–∞–∫—Å–∏–º—É–º: {stage_stats['Construction_Stage_Mean'].max():.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# üîÑ –≠–¢–ê–ü 2: –°–ë–û–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê –î–õ–Ø –ú–û–î–ï–õ–ò\n",
    "# ==============================================================================\n",
    "# –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ 1-–π –≥–æ–¥ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "df_modeling = df_all[df_all[\"Year_Num\"] == 1].copy()\n",
    "\n",
    "# –ú–µ—Ä–∂–∏–º –Ω–∞—à—É –Ω–æ–≤—É—é —Ñ–∏—á—É\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º inner merge, —á—Ç–æ–±—ã –æ—Å—Ç–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–æ–µ–∫—Ç—ã, –≥–¥–µ –µ—Å—Ç—å –∏ —Ñ–∏—á–∏, –∏ —Å—Ç–∞–¥–∏—è\n",
    "df_modeling = df_modeling.merge(stage_stats, on=\"Project\", how=\"inner\")\n",
    "\n",
    "print(f\"\\nüìä –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {len(df_modeling)} —Å—Ç—Ä–æ–∫.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# üî¨ –≠–¢–ê–ü 3: –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø\n",
    "# ==============================================================================\n",
    "def run_stage_model(df, features, target):\n",
    "    # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "    data = df.dropna(subset=features + [target])\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ OLS\n",
    "    X = sm.add_constant(data[features])\n",
    "    y = data[target]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # --- –¢–ï–ö–°–¢–û–í–û–ô –û–¢–ß–ï–¢ ---\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üî¨ –ú–û–î–ï–õ–¨: –í–ª–∏—è–Ω–∏–µ –°—Ç–∞–¥–∏–∏ –°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ + –ë–∞–Ω–∫–∏ + –ú–∞–∫—Ä–æ (1 –ì–æ–¥)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"üìå R-squared (–¢–æ—á–Ω–æ—Å—Ç—å):      {model.rsquared:.4f}\")\n",
    "    print(f\"üìå Adj. R-squared (–ß–µ—Å—Ç–Ω—ã–π):  {model.rsquared_adj:.4f}\")\n",
    "    print(f\"üìå F-pvalue:                  {model.f_pvalue:.4e}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # –ö—Ä–∞—Å–∏–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç\": model.params,\n",
    "        \"Std. Error\": model.bse,\n",
    "        \"P-value\": model.pvalues,\n",
    "        \"–ó–Ω–∞—á–∏–º–æ—Å—Ç—å\": model.pvalues.apply(lambda p: \"‚≠ê‚≠ê‚≠ê\" if p<0.01 else (\"‚≠ê‚≠ê\" if p<0.05 else (\"‚≠ê\" if p<0.1 else \"‚ùå\")))\n",
    "    })\n",
    "    \n",
    "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏\n",
    "    name_map = {\n",
    "        \"const\": \"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞\",\n",
    "        \"Bank_Index_Weighted\": \"–í–µ—Å –ë–∞–Ω–∫–æ–≤ (Log)\",\n",
    "        \"Construction_Stage_Mean\": \"–°—Ç–∞–¥–∏—è –°—Ç—Ä–æ–π–∫–∏ (0..1)\",\n",
    "        \"KR_Mean\": \"–°—Ä. –ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞\"\n",
    "    }\n",
    "    coef_df.index = coef_df.index.map(lambda x: name_map.get(x, x))\n",
    "    print(coef_df)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # --- –ì–†–ê–§–ò–ö–ò (3 –®—Ç—É–∫–∏ –∫–∞–∫ –≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö) ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    plt.suptitle(\"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏ (–° —É—á–µ—Ç–æ–º —Å—Ç–∞–¥–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞)\", fontsize=14, weight='bold')\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 1: –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑\n",
    "    preds = model.predict(X)\n",
    "    axes[0].scatter(y, preds, alpha=0.6, edgecolors='white', s=60, color='#2b7bba')\n",
    "    axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='–ò–¥–µ–∞–ª')\n",
    "    axes[0].set_xlabel(\"–†–µ–∞–ª—å–Ω—ã–π Sellout (%)\")\n",
    "    axes[0].set_ylabel(\"–ü—Ä–æ–≥–Ω–æ–∑ –ú–æ–¥–µ–ª–∏ (%)\")\n",
    "    axes[0].set_title(f\"–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∞\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 2: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
    "    residuals = model.resid\n",
    "    sns.histplot(residuals, kde=True, ax=axes[1], color='orange')\n",
    "    axes[1].axvline(0, color='red', linestyle='--')\n",
    "    axes[1].set_title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –û—à–∏–±–æ–∫\")\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 3: Q-Q Plot\n",
    "    sm.qqplot(residuals, line='45', fit=True, ax=axes[2])\n",
    "    axes[2].set_title(\"Q-Q Plot (–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –í–´–í–û–î ---\n",
    "    st_coef = model.params.get('Construction_Stage_Mean', 0)\n",
    "    st_pval = model.pvalues.get('Construction_Stage_Mean', 1)\n",
    "    \n",
    "    print(\"\\nüí° –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –í–´–í–û–î –ü–û –°–¢–ê–î–ò–ò:\")\n",
    "    if st_pval > 0.1:\n",
    "        print(\"   ‚ùå –°—Ç–∞–¥–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ù–ï –Ø–í–õ–Ø–ï–¢–°–Ø –∑–Ω–∞—á–∏–º—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º (p-value > 0.1).\")\n",
    "        print(\"      –í–æ–∑–º–æ–∂–Ω–æ, –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º –≤ —ç—Ç–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –Ω–µ —Ç–∞–∫ –≤–∞–∂–Ω–æ, –Ω–∞ –∫–∞–∫–æ–º —ç—Ç–∞–ø–µ —Å—Ç—Ä–æ–π–∫–∞,\")\n",
    "        print(\"      –ª–∏–±–æ —ç—Ñ—Ñ–µ–∫—Ç '–∫–æ—Ç–ª–æ–≤–∞–Ω–∞' (–¥–µ—à–µ–≤–æ) —É—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç '–≥–æ—Ç–æ–≤–æ–≥–æ' (–Ω–∞–¥–µ–∂–Ω–æ).\")\n",
    "    else:\n",
    "        direction = \"–ü–û–õ–û–ñ–ò–¢–ï–õ–¨–ù–û–ï\" if st_coef > 0 else \"–û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–û–ï\"\n",
    "        effect = \"–õ—É—á—à–µ –ø—Ä–æ–¥–∞–µ—Ç—Å—è –ì–û–¢–û–í–û–ï –∂–∏–ª—å–µ.\" if st_coef > 0 else \"–õ—É—á—à–µ –ø—Ä–æ–¥–∞–µ—Ç—Å—è –ö–û–¢–õ–û–í–ê–ù (–∏–Ω–≤–µ—Å—Ç-—Å–ø—Ä–æ—Å).\"\n",
    "        print(f\"   ‚úÖ –°—Ç–∞–¥–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏–º–æ–µ {direction} –≤–ª–∏—è–Ω–∏–µ.\")\n",
    "        print(f\"      –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {effect}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "run_stage_model(df_modeling, MODEL_FEATURES, TARGET_VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ffdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. –°–õ–û–í–ê–†–¨ –†–ï–ô–¢–ò–ù–ì–û–í (ERZ)\n",
    "# ==============================================================================\n",
    "erz_rating_dict = {\n",
    "    '–ê101': 5, \n",
    "    '–ê–±—Å–æ–ª—é—Ç –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å': 4.5, \n",
    "    '–ì–æ—Ä–∞–∫—Å': -1, \n",
    "    '–°–∞–º–æ–ª–µ—Ç': 4, \n",
    "    '–ü–ò–ö': 5,\n",
    "    '–§–°–ö / –î–°–ö-1': 4, \n",
    "    '–¶–µ–Ω—Ç—Ä—Å—Ç—Ä–æ–π': 0.5, \n",
    "    '–ò–Ω–≤–µ—Å—Ç—Ç—Ä–∞—Å—Ç': 4, \n",
    "    '–£–≠–ó': 0.5,\n",
    "    '–°—Ç—Ä–æ–π–∫–æ–º': -1, \n",
    "    '–†–û–°–¢': -1, \n",
    "    'Sezar Group': 5, \n",
    "    '3S GROUP': 1.5, \n",
    "    '–†–æ–¥–∏–Ω–∞': -1,\n",
    "    'Plus Development': -1, \n",
    "    'INGRAD': -1, \n",
    "    '–ù–æ–≤—ã–µ –∫–≤–∞—Ä—Ç–∞–ª—ã': 0, \n",
    "    '–ì—Ä—É–ø–ø–∞ –ê–∫–≤–∏–ª–æ–Ω': 4.5,\n",
    "    '–ö—Ä–æ—Å—Ç': 1.5, \n",
    "    '–ê—Ç–ª–∞–Ω—Ç': -1, \n",
    "    'Barkli': 0.5, \n",
    "    '–í—ã–±–æ—Ä': 5, \n",
    "    'Unikey': 5\n",
    "}\n",
    "\n",
    "def analyze_rating_robustness_separate_years(df_main, path_proj_file):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"üõ°Ô∏è –ü–†–û–í–ï–†–ö–ê –ù–ê –£–°–¢–û–ô–ß–ò–í–û–°–¢–¨: –†–ê–ó–ë–ò–ï–ù–ò–ï –ü–û –†–ï–ô–¢–ò–ù–ì–£ –î–ï–í–ï–õ–û–ü–ï–†–ê\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # --- –®–ê–ì 1: –û–ë–û–ì–ê–©–ï–ù–ò–ï –î–ê–ù–ù–´–• ---\n",
    "    try:\n",
    "        raw_proj = pd.read_excel(path_proj_file)\n",
    "        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –¥–µ–≤–µ–ª–æ–ø–µ—Ä–æ–º\n",
    "        dev_col = next((col for col in raw_proj.columns if \"–¥–µ–≤–µ–ª–æ–ø–µ—Ä\" in col.lower() or \"–∑–∞—Å—Ç—Ä–æ–π—â–∏–∫\" in col.lower()), None)\n",
    "        \n",
    "        if not dev_col:\n",
    "            print(\"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '–î–µ–≤–µ–ª–æ–ø–µ—Ä' –≤ —Ñ–∞–π–ª–µ –ø—Ä–æ–µ–∫—Ç–æ–≤.\")\n",
    "            return\n",
    "\n",
    "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞\n",
    "        proj_dev_map = raw_proj[[\"–ü—Ä–æ–µ–∫—Ç\", dev_col]].drop_duplicates()\n",
    "        proj_dev_map[\"–ü—Ä–æ–µ–∫—Ç\"] = proj_dev_map[\"–ü—Ä–æ–µ–∫—Ç\"].astype(str).str.strip()\n",
    "        \n",
    "        # –ú–µ—Ä–¥–∂\n",
    "        df_rat = df_main.copy()\n",
    "        df_rat = df_rat.merge(proj_dev_map, left_on=\"Project\", right_on=\"–ü—Ä–æ–µ–∫—Ç\", how=\"left\")\n",
    "        \n",
    "        # –ú–∞–ø–ø–∏–Ω–≥ —Ä–µ–π—Ç–∏–Ω–≥–∞\n",
    "        df_rat[\"Developer_Rating\"] = df_rat[dev_col].map(erz_rating_dict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- –®–ê–ì 2: –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ---\n",
    "    # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö, –∫–æ–≥–æ –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –∏–ª–∏ —Ä–µ–π—Ç–∏–Ω–≥ -1\n",
    "    total_len = len(df_rat)\n",
    "    df_valid = df_rat[\n",
    "        (df_rat[\"Developer_Rating\"].notna()) & \n",
    "        (df_rat[\"Developer_Rating\"] != -1)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "    print(f\"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –Ω–∞ –≤—Ö–æ–¥–µ: {total_len}\")\n",
    "    print(f\"   –ò—Å–∫–ª—é—á–µ–Ω–æ (–†–µ–π—Ç–∏–Ω–≥ -1 –∏–ª–∏ –Ω–µ—Ç –≤ –±–∞–∑–µ): {total_len - len(df_valid)}\")\n",
    "    print(f\"   –ì–æ—Ç–æ–≤–æ –∫ –∞–Ω–∞–ª–∏–∑—É: {len(df_valid)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "    features = ['Bank_Index_Weighted', 'KR_Mean']\n",
    "    target = 'Sellout'\n",
    "    main_feature = 'Bank_Index_Weighted'\n",
    "\n",
    "    # --- –®–ê–ì 3: –¶–ò–ö–õ –ü–û –ì–û–î–ê–ú (1 –∏ 2) ---\n",
    "    for year in [1, 2]:\n",
    "        print(f\"\\n{'#'*40}\")\n",
    "        print(f\"üìÖ –ê–ù–ê–õ–ò–ó –ó–ê –ì–û–î {year}\")\n",
    "        print(f\"{'#'*40}\")\n",
    "        \n",
    "        # 3.1 –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≥–æ–¥–∞\n",
    "        df_year = df_valid[df_valid[\"Year_Num\"] == year].copy()\n",
    "        \n",
    "        if df_year.empty:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ì–æ–¥–∞ {year}\")\n",
    "            continue\n",
    "            \n",
    "        # 3.2 –°—á–∏—Ç–∞–µ–º –º–µ–¥–∏–∞–Ω—É –≤–Ω—É—Ç—Ä–∏ –≥–æ–¥–∞ (–∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ, –∑–¥–µ—Å—å –±–µ—Ä–µ–º –≤–Ω—É—Ç—Ä–∏ –≤—ã–±–æ—Ä–∫–∏ –≥–æ–¥–∞)\n",
    "        median_rating = df_year[\"Developer_Rating\"].median()\n",
    "        \n",
    "        # 3.3 –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã\n",
    "        df_low = df_year[df_year[\"Developer_Rating\"] <= median_rating].copy()\n",
    "        df_high = df_year[df_year[\"Developer_Rating\"] > median_rating].copy()\n",
    "        \n",
    "        # –õ–µ–π–±–ª—ã –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "        label_low = f\"–ù–∏–∑–∫–∏–π/–°—Ä–µ–¥–Ω–∏–π (<= {median_rating})\"\n",
    "        label_high = f\"–í—ã—Å–æ–∫–∏–π (> {median_rating})\"\n",
    "        \n",
    "        df_low[\"Group\"] = label_low\n",
    "        df_high[\"Group\"] = label_high\n",
    "        \n",
    "        print(f\"üîπ –ú–µ–¥–∏–∞–Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–∞: {median_rating}\")\n",
    "        print(f\"üîπ –ü—Ä–æ–µ–∫—Ç–æ–≤ —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º <= {median_rating}: {len(df_low)}\")\n",
    "        print(f\"üîπ –ü—Ä–æ–µ–∫—Ç–æ–≤ —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º >  {median_rating}: {len(df_high)}\")\n",
    "\n",
    "        # 3.4 –§—É–Ω–∫—Ü–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "        def run_model_local(sub_df, group_name):\n",
    "            sub_df = sub_df.dropna(subset=features + [target])\n",
    "            if sub_df.shape[0] < 5: # –ó–∞—â–∏—Ç–∞ –æ—Ç –º–∞–ª–æ–≥–æ –∫–æ–ª-–≤–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "                print(f\"   ‚ö†Ô∏è {group_name}: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏!\")\n",
    "                return\n",
    "            \n",
    "            X = sm.add_constant(sub_df[features])\n",
    "            y = sub_df[target]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            \n",
    "            coef = model.params.get(main_feature, 0)\n",
    "            pval = model.pvalues.get(main_feature, 1)\n",
    "            signif = \"‚úÖ –ó–ù–ê–ß–ò–ú–û\" if pval < 0.05 else \"‚ùå –°–õ–£–ß–ê–ô–ù–û\"\n",
    "            \n",
    "            print(f\"\\n   üèó –ú–û–î–ï–õ–¨: {group_name}\")\n",
    "            print(f\"      R¬≤: {model.rsquared:.4f}\")\n",
    "            print(f\"      –í–ª–∏—è–Ω–∏–µ –±–∞–Ω–∫–æ–≤ (Coef): {coef:.4f} [{signif}, p={pval:.4f}]\")\n",
    "            print(model.summary().tables[1])\n",
    "\n",
    "        # –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π\n",
    "        run_model_local(df_low, \"–ì–†–£–ü–ü–ê: –ù–∏–∑–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥\")\n",
    "        run_model_local(df_high, \"–ì–†–£–ü–ü–ê: –í—ã—Å–æ–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥\")\n",
    "\n",
    "        # 3.5 –ì–†–ê–§–ò–ö (–û–¢–î–ï–õ–¨–ù–´–ô –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ì–û–î–ê)\n",
    "        print(f\"\\n   üìà –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –ì–æ–¥–∞ {year}...\")\n",
    "        df_viz = pd.concat([df_low, df_high])\n",
    "        \n",
    "        # lmplot —Å–æ–∑–¥–∞–µ—Ç —Å–≤–æ—é —Ñ–∏–≥—É—Ä—É, –ø–æ—ç—Ç–æ–º—É plt.figure() –ø–µ—Ä–µ–¥ –Ω–∏–º –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é,\n",
    "        # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–º–µ—Ä–∞ –ø–µ—Ä–µ–¥–∞–µ–º –≤–Ω—É—Ç—Ä—å height/aspect\n",
    "        g = sns.lmplot(\n",
    "            data=df_viz, \n",
    "            x=main_feature, \n",
    "            y=target, \n",
    "            hue=\"Group\", \n",
    "            palette={label_low: \"orange\", label_high: \"green\"},\n",
    "            height=6, \n",
    "            aspect=1.6,\n",
    "            scatter_kws={'alpha': 0.6, 's': 70},\n",
    "            line_kws={'linewidth': 3}\n",
    "        )\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á–µ—Ä–µ–∑ –æ–±—ä–µ–∫—Ç FacetGrid\n",
    "        g.fig.suptitle(f\"[–ì–û–î {year}] –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –±–∞–Ω–∫–æ–≤: –†–µ–π—Ç–∏–Ω–≥ –¥–µ–≤–µ–ª–æ–ø–µ—Ä–æ–≤\", fontsize=16, y=1.02)\n",
    "        g.set_axis_labels(\"–í–µ—Å –±–∞–Ω–∫–æ–≤ (Log Index)\", \"–°–µ–ª–∞—É—Ç (%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show() # <--- –í–ê–ñ–ù–û: Show –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞, —á—Ç–æ–±—ã –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–µ —Å–ª–∏–ø–ª–∏—Å—å\n",
    "\n",
    "# ==============================================================================\n",
    "# –ó–ê–ü–£–°–ö\n",
    "# ==============================================================================\n",
    "# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ df_all –∏ FILE_PROJ —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤—ã—à–µ\n",
    "analyze_rating_robustness_separate_years(df_all, FILE_PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c256262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def analyze_district_robustness(df_main, path_deals_file, features, target=\"Sellout\"):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å: –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –¢–æ–ø-3 –û–∫—Ä—É–≥–∞–º.\n",
    "    1. –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ—Ç –û–∫—Ä—É–≥ –∏–∑ raw-—Ñ–∞–π–ª–∞ —Å–¥–µ–ª–æ–∫.\n",
    "    2. –í—ã–±–∏—Ä–∞–µ—Ç 3 –æ–∫—Ä—É–≥–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø—Ä–æ–µ–∫—Ç–æ–≤.\n",
    "    3. –°—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª–∏ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –±–∞–Ω–∫–æ–≤ –≤ –∫–∞–∂–¥–æ–º –æ–∫—Ä—É–≥–µ.\n",
    "    \"\"\"\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"üèôÔ∏è –ü–†–û–í–ï–†–ö–ê –ù–ê –£–°–¢–û–ô–ß–ò–í–û–°–¢–¨: –¢–û–ü-3 –û–ö–†–£–ì–ê (Location Robustness)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # --- –®–ê–ì 1: –ü–û–î–¢–Ø–ì–ò–í–ê–ï–ú –û–ö–†–£–ì–ê ---\n",
    "    try:\n",
    "        # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "        raw_deals = pd.read_excel(path_deals_file)\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (–æ–±—ã—á–Ω–æ '–û–∫—Ä—É–≥' –∏–ª–∏ '–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –æ–∫—Ä—É–≥')\n",
    "        col_district = next((c for c in raw_deals.columns if \"–æ–∫—Ä—É–≥\" in c.lower()), None)\n",
    "        \n",
    "        if not col_district:\n",
    "            print(\"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '–û–∫—Ä—É–≥' –≤ —Ñ–∞–π–ª–µ —Å–¥–µ–ª–æ–∫.\")\n",
    "            return\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥: –ü—Ä–æ–µ–∫—Ç -> –û–∫—Ä—É–≥ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–æ–ø–∞–≤—à–∏–π—Å—è, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ)\n",
    "        # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –º–µ—Ä–¥–∂–∞\n",
    "        raw_deals[\"–ü—Ä–æ–µ–∫—Ç\"] = raw_deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö\"].astype(str).str.strip()\n",
    "        proj_dist_map = raw_deals.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[col_district].first().reset_index()\n",
    "        proj_dist_map.columns = [\"Project\", \"District\"]\n",
    "        \n",
    "        # –ú–µ—Ä–¥–∂–∏–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º\n",
    "        df_geo = df_main.copy()\n",
    "        df_geo = df_geo.merge(proj_dist_map, on=\"Project\", how=\"left\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ —Å–¥–µ–ª–æ–∫: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- –®–ê–ì 2: –û–ü–†–ï–î–ï–õ–Ø–ï–ú –¢–û–ü-3 –û–ö–†–£–ì–ê ---\n",
    "    # –°—á–∏—Ç–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –≤ –∫–∞–∂–¥–æ–º –æ–∫—Ä—É–≥–µ\n",
    "    top_districts = df_geo.groupby(\"District\")[\"Project\"].nunique().nlargest(3).index.tolist()\n",
    "    \n",
    "    print(f\"üèÜ –¢–æ–ø-3 –æ–∫—Ä—É–≥–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–æ–µ–∫—Ç–æ–≤: {', '.join(top_districts)}\")\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç–∏ 3 –æ–∫—Ä—É–≥–∞)\n",
    "    df_geo = df_geo[df_geo[\"District\"].isin(top_districts)]\n",
    "    print(f\"üìä –°—Ç—Ä–æ–∫ –≤ –≤—ã–±–æ—Ä–∫–µ (–¢–æ–ø-3): {len(df_geo)}\")\n",
    "    \n",
    "    # –ì–ª–∞–≤–Ω–∞—è —Ñ–∏—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–µ—Ä–≤–æ–π –≤ —Å–ø–∏—Å–∫–µ features)\n",
    "    main_feature = features[0] \n",
    "\n",
    "    # --- –®–ê–ì 3: –¶–ò–ö–õ –ü–û –ì–û–î–ê–ú ---\n",
    "    for year in [1, 2]:\n",
    "        print(f\"\\n{'#'*40}\")\n",
    "        print(f\"üìÖ –ê–ù–ê–õ–ò–ó –ó–ê –ì–û–î {year}\")\n",
    "        print(f\"{'#'*40}\")\n",
    "        \n",
    "        df_year = df_geo[df_geo[\"Year_Num\"] == year].copy()\n",
    "        \n",
    "        if df_year.empty:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ì–æ–¥–∞ {year}\")\n",
    "            continue\n",
    "\n",
    "        # 3.1 –ó–∞–ø—É—Å–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫—Ä—É–≥–∞\n",
    "        for dist in top_districts:\n",
    "            sub_df = df_year[df_year[\"District\"] == dist].dropna(subset=features + [target])\n",
    "            \n",
    "            if len(sub_df) < 5:\n",
    "                print(f\"   ‚ö†Ô∏è {dist}: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(sub_df)} —à—Ç.)\")\n",
    "                continue\n",
    "                \n",
    "            X = sm.add_constant(sub_df[features])\n",
    "            y = sub_df[target]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            \n",
    "            coef = model.params.get(main_feature, 0)\n",
    "            pval = model.pvalues.get(main_feature, 1)\n",
    "            signif = \"‚úÖ –ó–ù–ê–ß–ò–ú–û\" if pval < 0.05 else \"‚ùå –°–õ–£–ß–ê–ô–ù–û\"\n",
    "            \n",
    "            print(f\"\\n   üìç –û–ö–†–£–ì: {dist}\")\n",
    "            print(f\"      R¬≤: {model.rsquared:.4f} | –ü—Ä–æ–µ–∫—Ç–æ–≤: {len(sub_df)}\")\n",
    "            print(f\"      –í–ª–∏—è–Ω–∏–µ ({main_feature}): {coef:.4f} [{signif}, p={pval:.4f}]\")\n",
    "            \n",
    "            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã, –≤—ã–≤–æ–¥–∏–º –∏—Ö –∫—Ä–∞—Ç–∫–æ\n",
    "            other_feats = [f for f in features if f != main_feature]\n",
    "            if other_feats:\n",
    "                sig_others = [f\"{f} ({model.params[f]:.2f})\" for f in other_feats if model.pvalues[f] < 0.1]\n",
    "                if sig_others:\n",
    "                    print(f\"      üìù –î–æ–ø. —Ñ–∞–∫—Ç–æ—Ä—ã (p<0.1): {', '.join(sig_others)}\")\n",
    "\n",
    "        # 3.2 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø (–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–∫–ª–æ–Ω–∞ –ø—Ä—è–º—ã—Ö)\n",
    "        print(f\"\\n   üìà –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–∫—Ä—É–≥–æ–≤ (–ì–æ–¥ {year})...\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lmplot(\n",
    "            data=df_year, \n",
    "            x=main_feature, \n",
    "            y=target, \n",
    "            hue=\"District\",\n",
    "            palette=\"bright\", # –Ø—Ä–∫–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–∏—è\n",
    "            height=6, \n",
    "            aspect=1.5,\n",
    "            scatter_kws={'alpha': 0.5, 's': 50},\n",
    "            line_kws={'linewidth': 3}\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"[–ì–û–î {year}] –í–ª–∏—è–Ω–∏–µ –±–∞–Ω–∫–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö –æ–∫—Ä—É–≥–∞—Ö –ú–æ—Å–∫–≤—ã\", fontsize=14)\n",
    "        plt.xlabel(f\"–ü—Ä–µ–¥–∏–∫—Ç–æ—Ä: {main_feature}\")\n",
    "        plt.ylabel(f\"–¶–µ–ª—å: {target}\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# –ó–ê–ü–£–°–ö\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ (–ú–û–ñ–ù–û –ú–ï–ù–Ø–¢–¨ –¢–£–¢)\n",
    "# –í–∞–∂–Ω–æ: –ü–µ—Ä–≤—ã–º —Å—Ç–∞–≤—å —Ç–æ—Ç —Ñ–∞–∫—Ç–æ—Ä, –≤–ª–∏—è–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ö–æ—á–µ—à—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ!\n",
    "features_for_geo = [\n",
    "    'Bank_Index_Weighted', # <--- –ì–ª–∞–≤–Ω—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–π —Ñ–∞–∫—Ç–æ—Ä\n",
    "    'KR_Mean',       # –ö–æ–Ω—Ç—Ä–æ–ª—å —Å—É–±—Å–∏–¥–∏–π\n",
    "    'Log_Area'             # –ö–æ–Ω—Ç—Ä–æ–ª—å –º–∞—Å—à—Ç–∞–±–∞\n",
    "]\n",
    "\n",
    "# 2. –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é (–∏—Å–ø–æ–ª—å–∑—É–µ–º df_all –∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–¥–µ–ª–æ–∫)\n",
    "analyze_district_robustness(df_all, FILE_DEALS, features_for_geo, target=\"Sellout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07970317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "# –ë–µ—Ä–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ df_all\n",
    "cols_base = ['Project', 'Year_Num', 'Sellout', 'Num_Banks_Filtered', 'Bank_Index_Weighted', 'Log_Area', 'KR_Mean']\n",
    "df_corr = df_all[cols_base].copy()\n",
    "df_corr = df_corr.rename(columns={'Num_Banks_Filtered': '–ö–æ–ª-–≤–æ –í–°–ï–• –±–∞–Ω–∫–æ–≤', 'Bank_Index_Weighted': '–í–µ—Å –í–°–ï–• –±–∞–Ω–∫–æ–≤'})\n",
    "\n",
    "# –ë–µ—Ä–µ–º –∫–æ–ª–æ–Ω–∫—É \"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ\" –∏–∑ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "df_temp = df_filtered[['Project', 'Year_Num', 'Num_Banks_Filtered']].copy()\n",
    "df_temp = df_temp.rename(columns={'Num_Banks_Filtered': '–ö–æ–ª-–≤–æ –¢–û–ü –±–∞–Ω–∫–æ–≤'})\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
    "df_corr_final = df_corr.merge(df_temp, on=['Project', 'Year_Num'], how='inner')\n",
    "\n",
    "# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã\n",
    "cols_for_heatmap = ['Sellout', '–ö–æ–ª-–≤–æ –í–°–ï–• –±–∞–Ω–∫–æ–≤', '–ö–æ–ª-–≤–æ –¢–û–ü –±–∞–Ω–∫–æ–≤', '–í–µ—Å –í–°–ï–• –±–∞–Ω–∫–æ–≤', 'Log_Area', 'KR_Mean']\n",
    "corr_matrix = df_corr_final[cols_for_heatmap].corr()\n",
    "\n",
    "# 2. –†–∏—Å—É–µ–º –≥—Ä–∞—Ñ–∏–∫\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=1, vmin=-1, vmax=1)\n",
    "plt.title(\"–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –±–∞–Ω–∫–æ–≤ –∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç –í–´–í–û–î: –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É '–ö–æ–ª-–≤–æ –í–°–ï–• –±–∞–Ω–∫–æ–≤' –∏ '–ö–æ–ª-–≤–æ –¢–û–ü –±–∞–Ω–∫–æ–≤'.\")\n",
    "print(\"   –ï—Å–ª–∏ –æ–Ω–∞ > 0.9, —Ç–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–∞–ª–æ –º–µ–Ω—è–µ—Ç —Å—É—Ç—å. –ï—Å–ª–∏ < 0.8 ‚Äî —Ä–∞–∑–Ω–∏—Ü–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615300f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def analyze_top_errors(df, features, target=\"Sellout\", year_label=\"\"):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å, –Ω–∞—Ö–æ–¥–∏—Ç 5 –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–æ–π –ø—Ä–æ–≥–Ω–æ–∑–∞ \n",
    "    –∏ –≤—ã–≤–æ–¥–∏—Ç –∏—Ö –ø–æ–¥—Ä–æ–±–Ω–æ–µ '–¥–æ—Å—å–µ'.\n",
    "    \"\"\"\n",
    "    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    data = df.copy()\n",
    "    data = data.dropna(subset=features + [target])\n",
    "    \n",
    "    # 2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    X = sm.add_constant(data[features])\n",
    "    y = data[target]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # 3. –†–∞—Å—á–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ –æ—à–∏–±–æ–∫\n",
    "    data['Predicted'] = model.predict(X)\n",
    "    data['Error'] = data[target] - data['Predicted'] # –û—à–∏–±–∫–∞ = –§–∞–∫—Ç - –ü—Ä–æ–≥–Ω–æ–∑\n",
    "    data['Abs_Error'] = data['Error'].abs()          # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏\n",
    "    \n",
    "    # 4. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤—ã–±–æ—Ä –¢–æ–ø-5\n",
    "    top_errors = data.sort_values('Abs_Error', ascending=False).head(5)\n",
    "    \n",
    "    # 5. –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"üö© –¢–û–ü-5 –ê–ù–û–ú–ê–õ–ò–ô (–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏) | {year_label}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    print(f\"R¬≤ –º–æ–¥–µ–ª–∏: {model.rsquared:.4f}\")\n",
    "    print(\"–ü–æ—è—Å–Ω–µ–Ω–∏–µ: \")\n",
    "    print(\"   ‚Ä¢ –û—à–∏–±–∫–∞ > 0: –ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–¥–∞–≤–∞–ª—Å—è –õ–£–ß–®–ï, —á–µ–º –æ–∂–∏–¥–∞–ª–∞ –º–æ–¥–µ–ª—å (–ù–µ–¥–æ–æ—Ü–µ–Ω–∫–∞).\")\n",
    "    print(\"   ‚Ä¢ –û—à–∏–±–∫–∞ < 0: –ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–¥–∞–≤–∞–ª—Å—è –•–£–ñ–ï, —á–µ–º –æ–∂–∏–¥–∞–ª–∞ –º–æ–¥–µ–ª—å (–ü–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞).\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å TRANS_MAP –∏–∑ 0-–π —è—á–µ–π–∫–∏, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤—å –∫–∞–∫ –µ—Å—Ç—å)\n",
    "    # –ó–¥–µ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏–º –ª–æ–∫–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã —è—á–µ–π–∫–∞ –±—ã–ª–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π\n",
    "    cols_map = {\n",
    "        'Project': '–ü–†–û–ï–ö–¢',\n",
    "        'Sellout': '–§–ê–ö–¢ (Sellout %)',\n",
    "        'Predicted': '–ü–†–û–ì–ù–û–ó –ú–æ–¥–µ–ª–∏',\n",
    "        'Error': '–û–®–ò–ë–ö–ê (–†–∞–∑–Ω–∏—Ü–∞)',\n",
    "        'Num_Banks_Filtered': '–ö–æ–ª-–≤–æ –ë–∞–Ω–∫–æ–≤',\n",
    "        'Bank_Index_Weighted': '–í–µ—Å –ë–∞–Ω–∫–æ–≤ (Log)',\n",
    "        'Log_Area': '–ú–∞—Å—à—Ç–∞–± (Log Area)',\n",
    "        'KR_Mean': '–°—Ä. –ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞',\n",
    "        'Subsidy_Share': '–î–æ–ª—è –ø–µ—Ä–∏–æ–¥–∞ —Å—É–±—Å–∏–¥–∏–π'\n",
    "    }\n",
    "\n",
    "    for i, (idx, row) in enumerate(top_errors.iterrows(), 1):\n",
    "        print(f\"\\nüèÜ –ú–ï–°–¢–û #{i}: {row['Project']} (–ö–ª–∞—Å—Å: {row.get('Class', 'N/A')})\")\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–∏–Ω–∏-—Ç–∞–±–ª–∏—á–∫—É –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "        stats_data = {\n",
    "            \"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å\": [],\n",
    "            \"–ó–Ω–∞—á–µ–Ω–∏–µ\": []\n",
    "        }\n",
    "        \n",
    "        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—à–∏–±–∫–∏\n",
    "        stats_data[\"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å\"].append(\"üî¥ –û–®–ò–ë–ö–ê (–§–∞–∫—Ç - –ü—Ä–æ–≥–Ω–æ–∑)\")\n",
    "        stats_data[\"–ó–Ω–∞—á–µ–Ω–∏–µ\"].append(f\"{row['Error']:+.2f} p.p.\")\n",
    "        \n",
    "        stats_data[\"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å\"].append(\"–§–ê–ö–¢ (–†–µ–∞–ª—å–Ω—ã–π Sellout)\")\n",
    "        stats_data[\"–ó–Ω–∞—á–µ–Ω–∏–µ\"].append(f\"{row[target]:.2f}%\")\n",
    "        \n",
    "        stats_data[\"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å\"].append(\"–ü–†–û–ì–ù–û–ó –ú–æ–¥–µ–ª–∏\")\n",
    "        stats_data[\"–ó–Ω–∞—á–µ–Ω–∏–µ\"].append(f\"{row['Predicted']:.2f}%\")\n",
    "        \n",
    "        # –§–∞–∫—Ç–æ—Ä—ã (–ø–æ—á–µ–º—É –º–æ–¥–µ–ª—å —Ç–∞–∫ —Ä–µ—à–∏–ª–∞)\n",
    "        for feat in features:\n",
    "            val = row[feat]\n",
    "            name = cols_map.get(feat, feat)\n",
    "            stats_data[\"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å\"].append(f\"   -> –§–∞–∫—Ç–æ—Ä: {name}\")\n",
    "            stats_data[\"–ó–Ω–∞—á–µ–Ω–∏–µ\"].append(f\"{val:.4f}\")\n",
    "\n",
    "        # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—á–∫–æ–π\n",
    "        row_df = pd.DataFrame(stats_data)\n",
    "        print(row_df.to_string(index=False, header=False, col_space=25, justify='left'))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# ==========================================\n",
    "# –ó–ê–ü–£–°–ö\n",
    "# ==========================================\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è \"–ü–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏\"\n",
    "features_check = [\n",
    "    'Num_Banks_Filtered',   # –ë–∞–Ω–∫–∏\n",
    "    'Log_Area',             # –ú–∞—Å—à—Ç–∞–±\n",
    "    'KR_Mean',              # –°—Ç–∞–≤–∫–∞ –¶–ë\n",
    "    'Subsidy_Share',        # –ì–æ—Å–ø—Ä–æ–≥—Ä–∞–º–º—ã\n",
    "    'Mortgage_Share'        # –î–æ–ª—è –∏–ø–æ—Ç–µ–∫–∏\n",
    "]\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π (–ò—Å–ø–æ–ª—å–∑—É–µ–º df_all - –æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç)\n",
    "# –ï—Å–ª–∏ df_all –Ω–µ —Å–æ–∑–¥–∞–Ω, –∑–∞–º–µ–Ω–∏ –Ω–∞ df_ml\n",
    "target_df = df_all\n",
    "\n",
    "# 1. –ê–Ω–æ–º–∞–ª–∏–∏ –ü–µ—Ä–≤–æ–≥–æ –≥–æ–¥–∞\n",
    "analyze_top_errors(target_df[target_df[\"Year_Num\"]==1], features_check, \"Sellout\", \"–ì–û–î 1\")\n",
    "\n",
    "# 2. –ê–Ω–æ–º–∞–ª–∏–∏ –í—Ç–æ—Ä–æ–≥–æ –≥–æ–¥–∞\n",
    "analyze_top_errors(target_df[target_df[\"Year_Num\"]==2], features_check, \"Sellout\", \"–ì–û–î 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def analyze_size_split_by_year(df, features, target=\"Sellout\"):\n",
    "    \"\"\"\n",
    "    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ–¥–∞ (1 –∏ 2):\n",
    "    1. –î–µ–ª–∏—Ç –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞ Small/Large –ø–æ –º–µ–¥–∏–∞–Ω–µ –ø–ª–æ—â–∞–¥–∏.\n",
    "    2. –°—Ç—Ä–æ–∏—Ç –º–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã.\n",
    "    3. –†–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–∞–∫–ª–æ–Ω–∞ –ø—Ä—è–º—ã—Ö.\n",
    "    \"\"\"\n",
    "    \n",
    "    # –ì–ª–∞–≤–Ω–∞—è —Ñ–∏—á–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–µ—Ä–≤–æ–π –≤ —Å–ø–∏—Å–∫–µ features)\n",
    "    main_feature = \"Bank_Index_Weighted\" \n",
    "    \n",
    "    for year in [1, 2]:\n",
    "        # 1. –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≥–æ–¥\n",
    "        df_year = df[df[\"Year_Num\"] == year].copy()\n",
    "        \n",
    "        if df_year.empty:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ì–æ–¥–∞ {year}\")\n",
    "            continue\n",
    "            \n",
    "        # 2. –°—á–∏—Ç–∞–µ–º –º–µ–¥–∏–∞–Ω—É –∏ –¥–µ–ª–∏–º\n",
    "        median_area = df_year[\"Planned_Area\"].median()\n",
    "        \n",
    "        df_small = df_year[df_year[\"Planned_Area\"] <= median_area].copy()\n",
    "        df_large = df_year[df_year[\"Planned_Area\"] > median_area].copy()\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É –≥—Ä—É–ø–ø—ã –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "        df_small[\"Group\"] = \"–ú–∞–ª–µ–Ω—å–∫–∏–µ (Red)\"\n",
    "        df_large[\"Group\"] = \"–ë–æ–ª—å—à–∏–µ (Blue)\"\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"üìÖ –ê–ù–ê–õ–ò–ó –ó–ê –ì–û–î {year} | –ú–µ–¥–∏–∞–Ω–∞ –ø–ª–æ—â–∞–¥–∏: {median_area:,.0f} –º¬≤\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        print(f\"üîπ –ú–∞–ª–µ–Ω—å–∫–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(df_small)}\")\n",
    "        print(f\"üîπ –ë–æ–ª—å—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤:   {len(df_large)}\")\n",
    "\n",
    "        # 3. –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–ú–ù–û–ì–û–§–ê–ö–¢–û–†–ù–ê–Ø)\n",
    "        def run_model(sub_df, group_name):\n",
    "            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "            sub_df = sub_df.dropna(subset=features + [target])\n",
    "            X = sm.add_constant(sub_df[features])\n",
    "            y = sub_df[target]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            \n",
    "            print(f\"\\nüìä --- {group_name} (–ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å) ---\")\n",
    "            print(f\"   R¬≤: {model.rsquared:.4f} | F-pvalue: {model.f_pvalue:.4e}\")\n",
    "            \n",
    "            # –í—ã–≤–æ–¥–∏–º –≤–ª–∏—è–Ω–∏–µ –í–ï–°–ê –ë–ê–ù–ö–û–í\n",
    "            if main_feature in model.params:\n",
    "                coef = model.params[main_feature]\n",
    "                pval = model.pvalues[main_feature]\n",
    "                signif = \"‚úÖ –ó–ù–ê–ß–ò–ú–û\" if pval < 0.05 else \"‚ùå –°–õ–£–ß–ê–ô–ù–û\"\n",
    "                print(f\"   üéØ –ö–æ—ç—Ñ. {main_feature}: {coef:.4f} [{signif}, p={pval:.4f}]\")\n",
    "            \n",
    "            # –í—ã–≤–æ–¥–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (–∫—Ä–∞—Ç–∫–æ)\n",
    "            print(\"   üìù –î—Ä—É–≥–∏–µ –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (p<0.05):\")\n",
    "            found = False\n",
    "            for feat in features:\n",
    "                if feat != main_feature and model.pvalues[feat] < 0.05:\n",
    "                    print(f\"      ‚Ä¢ {feat}: {model.params[feat]:.4f}\")\n",
    "                    found = True\n",
    "            if not found: print(\"      (–Ω–µ—Ç)\")\n",
    "            \n",
    "            return model\n",
    "\n",
    "        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "        run_model(df_small, \"SMALL (–ú–∞–ª–µ–Ω—å–∫–∏–µ)\")\n",
    "        run_model(df_large, \"LARGE (–ë–æ–ª—å—à–∏–µ)\")\n",
    "        \n",
    "        # 4. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø\n",
    "        print(f\"\\nüìà –ì–†–ê–§–ò–ö –ì–û–î–ê {year}: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤\")\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è —Ä–∏—Å–æ–≤–∞–Ω–∏—è\n",
    "        df_viz = pd.concat([df_small, df_large])\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # –†–∏—Å—É–µ–º –ª–∏–Ω–∏–∏ —Ç—Ä–µ–Ω–¥–∞ (lmplot –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–∞—Ä–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é)\n",
    "        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ì—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç \"—Å—ã—Ä—É—é\" —Å–≤—è–∑—å, –∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã—à–µ ‚Äî –æ—á–∏—â–µ–Ω–Ω—É—é (—Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)\n",
    "        sns.lmplot(\n",
    "            data=df_viz, \n",
    "            x=main_feature, \n",
    "            y=target, \n",
    "            hue=\"Group\", \n",
    "            palette={\"–ú–∞–ª–µ–Ω—å–∫–∏–µ (Red)\": \"red\", \"–ë–æ–ª—å—à–∏–µ (Blue)\": \"blue\"},\n",
    "            height=6, \n",
    "            aspect=1.5,\n",
    "            scatter_kws={'alpha': 0.6, 's': 60},\n",
    "            line_kws={'linewidth': 3}\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"[–ì–û–î {year}] –í–ª–∏—è–Ω–∏–µ –≤–µ—Å–∞ –±–∞–Ω–∫–æ–≤: –ú–∞–ª–µ–Ω—å–∫–∏–µ vs –ë–æ–ª—å—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã\", fontsize=14)\n",
    "        plt.xlabel(\"–í–µ—Å –±–∞–Ω–∫–æ–≤ (Log Index)\")\n",
    "        plt.ylabel(\"–°–µ–ª–∞—É—Ç (%)\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# –ó–ê–ü–£–°–ö\n",
    "# ==========================================\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º df_all (–æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç)\n",
    "target_df = df_all if 'df_all' in locals() else df_ml\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
    "# –í–∞–∂–Ω–æ: Bank_Index_Weighted –¥–æ–ª–∂–µ–Ω –∏–¥—Ç–∏ –ü–ï–†–í–´–ú, —Ç–∞–∫ –∫–∞–∫ –ø–æ –Ω–µ–º—É —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫\n",
    "features_robust = [\n",
    "    'Bank_Index_Weighted', # <--- –ì–ª–∞–≤–Ω–∞—è\n",
    "    'KR_Mean',             # –ö–æ–Ω—Ç—Ä–æ–ª—å –º–∞–∫—Ä–æ\n",
    "    'Subsidy_Share',       # –ö–æ–Ω—Ç—Ä–æ–ª—å –≥–æ—Å–ø—Ä–æ–≥—Ä–∞–º–º\n",
    "    'Mortgage_Share'       # –ö–æ–Ω—Ç—Ä–æ–ª—å —Ç–∏–ø–∞ —Å–¥–µ–ª–æ–∫\n",
    "    # Log_Area —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —É–±—Ä–∞–ª–∏, —Ç–∞–∫ –∫–∞–∫ –º—ã —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –≤—ã–±–æ—Ä–∫—É –ø–æ –ø–ª–æ—â–∞–¥–∏\n",
    "]\n",
    "\n",
    "analyze_size_split_by_year(target_df, features_robust, \"Sellout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f68f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def analyze_lagged_effect(df, include_autoregression=False):\n",
    "    \"\"\"\n",
    "    –°—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–¥–∞–∂ 2-–≥–æ –≥–æ–¥–∞ –æ—Ç –±–∞–Ω–∫–æ–≤ 1-–≥–æ –≥–æ–¥–∞.\n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä include_autoregression=False –æ—Ç–∫–ª—é—á–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞.\n",
    "    \"\"\"\n",
    "    mode_text = \"–° –ê–í–¢–û–†–ï–ì–†–ï–°–°–ò–ï–ô\" if include_autoregression else \"–ë–ï–ó –ê–í–¢–û–†–ï–ì–†–ï–°–°–ò–ò\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚è≥ –õ–ê–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó ({mode_text}): –í–ª–∏—è–Ω–∏–µ –±–∞–Ω–∫–æ–≤ 1-–≥–æ –≥–æ–¥–∞ –Ω–∞ 2-–π –≥–æ–¥\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    # –î–∞–Ω–Ω—ã–µ 1-–≥–æ –≥–æ–¥–∞ (–ü—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã)\n",
    "    df_y1 = df[df[\"Year_Num\"] == 1][['Project', 'Num_Banks_Filtered', 'Bank_Index_Weighted', 'Sellout']].copy()\n",
    "    df_y1 = df_y1.rename(columns={\n",
    "        'Bank_Index_Weighted': 'Bank_Weight_Y1', # –ù–∞—à –≥–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π\n",
    "        'Sellout': 'Sellout_Y1'                  # –£—Å–ø–µ—Ö –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞\n",
    "    })\n",
    "    \n",
    "    # –î–∞–Ω–Ω—ã–µ 2-–≥–æ –≥–æ–¥–∞ (–¢–∞—Ä–≥–µ—Ç + –ú–∞–∫—Ä–æ)\n",
    "    df_y2 = df[df[\"Year_Num\"] == 2][['Project', 'Sellout', 'KR_Mean', 'Subsidy_Share', 'Log_Area']].copy()\n",
    "    df_y2 = df_y2.rename(columns={'Sellout': 'Sellout_Y2'})\n",
    "    \n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º (Intersection)\n",
    "    df_lag = df_y1.merge(df_y2, on='Project', how='inner')\n",
    "    \n",
    "    # 2. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π\n",
    "    features = ['Bank_Weight_Y1', 'KR_Mean', 'Subsidy_Share']\n",
    "    \n",
    "    if include_autoregression:\n",
    "        features.append('Sellout_Y1') # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ø–µ—Ö –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞, –µ—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "    target = 'Sellout_Y2'\n",
    "    df_lag = df_lag.dropna(subset=features + [target])\n",
    "    \n",
    "    # 3. –û–±—É—á–µ–Ω–∏–µ\n",
    "    X = sm.add_constant(df_lag[features])\n",
    "    y = df_lag[target]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # 4. –¢–ï–ö–°–¢–û–í–û–ô –û–¢–ß–ï–¢\n",
    "    print(f\"üìå R-squared: {model.rsquared:.4f}\")\n",
    "    print(f\"üìå –ü—Ä–æ–µ–∫—Ç–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ: {len(df_lag)}\")\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        \"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç\": model.params,\n",
    "        \"P-–∑–Ω–∞—á–µ–Ω–∏–µ\": model.pvalues,\n",
    "        \"–ó–Ω–∞—á–∏–º–æ—Å—Ç—å\": model.pvalues.apply(lambda p: \"‚≠ê‚≠ê‚≠ê\" if p<0.001 else (\"‚≠ê‚≠ê\" if p<0.01 else (\"‚≠ê\" if p<0.05 else \"‚ùå\")))\n",
    "    })\n",
    "    \n",
    "    # –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–π\n",
    "    names_map = {\n",
    "        \"const\": \"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞\",\n",
    "        \"Bank_Weight_Y1\": \"–í–µ—Å –ë–∞–Ω–∫–æ–≤ (–í –ì–û–î 1)\",\n",
    "        \"Sellout_Y1\": \"–£—Å–ø–µ—Ö –ø—Ä–æ–¥–∞–∂ 1-–≥–æ –≥–æ–¥–∞\",\n",
    "        \"KR_Mean\": \"–ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞ (–≤–æ 2-–π –≥–æ–¥)\",\n",
    "        \"Subsidy_Share\": \"–î–æ–ª—è —Å—É–±—Å–∏–¥–∏–π (–≤–æ 2-–π –≥–æ–¥)\"\n",
    "    }\n",
    "    summary_df.index = summary_df.index.map(lambda x: names_map.get(x, x))\n",
    "    \n",
    "    print(\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
    "    print(summary_df)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 5. –ì–†–ê–§–ò–ö–ò (3 –®—Ç—É–∫–∏: –ü—Ä–æ–≥–Ω–æ–∑, –û—Å—Ç–∞—Ç–∫–∏, QQ)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    plt.suptitle(f\"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –õ–∞–≥–æ–≤–æ–π –ú–æ–¥–µ–ª–∏ ({mode_text})\", fontsize=14, weight='bold')\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 1: –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑\n",
    "    preds = model.predict(X)\n",
    "    axes[0].scatter(y, preds, alpha=0.6, edgecolors='b')\n",
    "    axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel(\"–†–µ–∞–ª—å–Ω—ã–π Sellout (–ì–æ–¥ 2)\")\n",
    "    axes[0].set_ylabel(\"–ü—Ä–æ–≥–Ω–æ–∑ –ú–æ–¥–µ–ª–∏\")\n",
    "    axes[0].set_title(\"–¢–æ—á–Ω–æ—Å—Ç—å –ü—Ä–æ–≥–Ω–æ–∑–∞\")\n",
    "\n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 2: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
    "    residuals = model.resid\n",
    "    sns.histplot(residuals, kde=True, ax=axes[1], color='orange')\n",
    "    axes[1].set_title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –û—à–∏–±–æ–∫\")\n",
    "    axes[1].axvline(0, color='red', linestyle='--')\n",
    "\n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 3: QQ-Plot\n",
    "    sm.qqplot(residuals, line='45', fit=True, ax=axes[2])\n",
    "    axes[2].set_title(\"Q-Q Plot (–ù–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# –ó–ê–ü–£–°–ö\n",
    "# ==========================================\n",
    "# –í—ã–±–∏—Ä–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–ª—É—á—à–µ df_all, —á—Ç–æ–±—ã –±—ã–ª–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª–∞–≥–æ–≤)\n",
    "target_df = df_all if 'df_all' in locals() else df_ml\n",
    "\n",
    "# 1. –ó–ê–ü–£–°–ö –ë–ï–ó –ê–í–¢–û–†–ï–ì–†–ï–°–°–ò–ò (–ß–∏—Å—Ç–æ–µ –≤–ª–∏—è–Ω–∏–µ –±–∞–Ω–∫–æ–≤)\n",
    "analyze_lagged_effect(target_df, include_autoregression=True)\n",
    "\n",
    "# 2. (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û) –ó–ê–ü–£–°–ö –° –ê–í–¢–û–†–ï–ì–†–ï–°–°–ò–ï–ô \n",
    "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π —Å—Ç—Ä–æ–∫—É –Ω–∏–∂–µ, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å —É–≤–∏–¥–µ—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–¥–∞–∂ –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞\n",
    "# analyze_lagged_effect(target_df, include_autoregression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "FILE_PROJ = \"–ü—Ä–æ–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ_2025-11-25.xlsx\"\n",
    "FILE_DEALS = \"–°–¥–µ–ª–∫–∏_2025-11-25.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31186a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_and_bank_weights(path_proj, path_deals):\n",
    "    print(\"üöÄ –≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –±–∞–Ω–∫–æ–≤...\")\n",
    "    \n",
    "    # 1. –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "    proj = pd.read_excel(path_proj)\n",
    "    deals = pd.read_excel(path_deals)\n",
    "\n",
    "    # 2. –û—á–∏—Å—Ç–∫–∞ —á–∏—Å–µ–ª\n",
    "    def clean_num(x):\n",
    "        return pd.to_numeric(str(x).replace(\"\\u00a0\", \"\").replace(\" \", \"\").replace(\",\", \".\"), errors=\"coerce\") or 0\n",
    "\n",
    "    for col in [\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\", \"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"]:\n",
    "        if col in proj.columns: proj[col] = proj[col].apply(clean_num)\n",
    "        if col in deals.columns: deals[col] = deals[col].apply(clean_num)\n",
    "\n",
    "    # 3. –û—á–∏—Å—Ç–∫–∞ ID –∏ –î–∞—Ç\n",
    "    for df in [proj, deals]:\n",
    "        df['ID –∫–æ—Ä–ø—É—Å–∞'] = df['ID –∫–æ—Ä–ø—É—Å–∞'].astype(str).str.replace(r'\\.0$', '', regex=True).str.strip()\n",
    "\n",
    "    deals[\"dt_deal\"] = pd.to_datetime(deals[\"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)\"], dayfirst=True, errors=\"coerce\")\n",
    "    mask_na = deals[\"dt_deal\"].isna()\n",
    "    if mask_na.any():\n",
    "        deals.loc[mask_na, \"dt_deal\"] = pd.to_datetime(\n",
    "            \"01.\" + deals.loc[mask_na, \"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)\"].astype(str), dayfirst=True, errors=\"coerce\")\n",
    "    deals = deals.dropna(subset=[\"dt_deal\"])\n",
    "\n",
    "    # 4. –ú—ç–ø–ø–∏–Ω–≥ –ø—Ä–æ–µ–∫—Ç–æ–≤\n",
    "    corpus_map = proj[[\"ID –∫–æ—Ä–ø—É—Å–∞\", \"–ü—Ä–æ–µ–∫—Ç\"]].drop_duplicates()\n",
    "    deals = deals.merge(corpus_map, on=\"ID –∫–æ—Ä–ø—É—Å–∞\", how=\"left\")\n",
    "    deals = deals.dropna(subset=[\"–ü—Ä–æ–µ–∫—Ç\"])\n",
    "\n",
    "    # 5. –†–ê–°–ß–ï–¢ –í–ï–°–û–í –ë–ê–ù–ö–û–í (–ì–ª–æ–±–∞–ª—å–Ω—ã–π)\n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è\n",
    "    deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"] = deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].fillna(\"–ù–µ —É–∫–∞–∑–∞–Ω\").astype(str).str.strip()\n",
    "    \n",
    "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –±–∞–Ω–∫–∞–º\n",
    "    bank_stats = deals.groupby(\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\")[\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"].sum().reset_index()\n",
    "    \n",
    "    # –°—á–∏—Ç–∞–µ–º Log-–≤–µ—Å (—Å–≥–ª–∞–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –≥–∏–≥–∞–Ω—Ç–∞–º–∏ –∏ —Å–µ—Ä–µ–¥–Ω—è–∫–∞–º–∏)\n",
    "    bank_stats[\"Bank_Weight\"] = np.log1p(bank_stats[\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"])\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –≤–µ—Å–æ–≤\n",
    "    weights_dict = bank_stats.set_index(\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\")[\"Bank_Weight\"].to_dict()\n",
    "    \n",
    "    print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞–Ω–∫–æ–≤: {len(bank_stats)}\")\n",
    "    print(f\"   –¢–æ–ø-–±–∞–Ω–∫ –ø–æ –≤–µ—Å—É: {bank_stats.sort_values('Bank_Weight', ascending=False).iloc[0]['–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞']}\")\n",
    "    \n",
    "    return proj, deals, weights_dict\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫\n",
    "proj_df, deals_df, bank_weights_map = prepare_data_and_bank_weights(FILE_PROJ, FILE_DEALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_incremental_dataset(proj, deals, weights_map, max_horizon=12):\n",
    "    print(f\"\\nüöÄ –≠–¢–ê–ü 2: –°–±–æ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ì–æ—Ä–∏–∑–æ–Ω—Ç: {max_horizon} –º–µ—Å.)\")\n",
    "    \n",
    "    # –°—Ç–∞—Ä—Ç –ø—Ä–æ–¥–∞–∂ –∏ –ø–ª–æ—â–∞–¥–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤\n",
    "    proj_starts = deals.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[\"dt_deal\"].min().reset_index().rename(columns={\"dt_deal\": \"start_date\"})\n",
    "    proj_areas = proj.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"].sum().reset_index()\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for project in proj_starts[\"–ü—Ä–æ–µ–∫—Ç\"].unique():\n",
    "        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞\n",
    "        start_date = proj_starts.loc[proj_starts[\"–ü—Ä–æ–µ–∫—Ç\"] == project, \"start_date\"].values[0]\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        \n",
    "        area_rows = proj_areas.loc[proj_areas[\"–ü—Ä–æ–µ–∫—Ç\"] == project, \"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"]\n",
    "        if area_rows.empty: continue\n",
    "        total_area = area_rows.values[0]\n",
    "        if total_area < 100: continue # –ü—Ä–æ–ø—É—Å–∫ –º—É—Å–æ—Ä–∞\n",
    "            \n",
    "        # –°–¥–µ–ª–∫–∏ –ø–æ —ç—Ç–æ–º—É –ø—Ä–æ–µ–∫—Ç—É\n",
    "        p_deals = deals[deals[\"–ü—Ä–æ–µ–∫—Ç\"] == project].copy()\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –º–µ—Å—è—Ü–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–¥–µ–ª–∫–∏ (0, 1, 2...)\n",
    "        p_deals[\"Month_Num\"] = (\n",
    "            (p_deals[\"dt_deal\"].dt.year - start_date.year) * 12 + \n",
    "            (p_deals[\"dt_deal\"].dt.month - start_date.month)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # –¶–∏–∫–ª –ø–æ –º–µ—Å—è—Ü–∞–º (–æ—Ç 0 –¥–æ max_horizon)\n",
    "        # –ú—ã —Å—Ç–æ–∏–º –≤ –º–µ—Å—è—Ü–µ 'm' –∏ –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–æ–¥–∞–∂–∏ –≤ 'm+1'\n",
    "        \n",
    "        accumulated_banks = set() # –°–µ—Ç –±–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –£–ñ–ï –∑–∞—à–ª–∏ –≤ –ø—Ä–æ–µ–∫—Ç\n",
    "        \n",
    "        for m in range(max_horizon):\n",
    "            # 1. –§–ò–ß–ò: –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –±–∞–Ω–∫–∏ –∫ –∫–æ–Ω—Ü—É –º–µ—Å—è—Ü–∞ m\n",
    "            # –ë–µ—Ä–µ–º –≤—Å–µ —Å–¥–µ–ª–∫–∏ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ\n",
    "            past_deals = p_deals[p_deals[\"Month_Num\"] <= m]\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º –∏–ø–æ—Ç–µ—á–Ω—ã–µ —Å–¥–µ–ª–∫–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å–µ—Ç –±–∞–Ω–∫–æ–≤\n",
    "            mort_deals = past_deals[past_deals[\"–ò–ø–æ—Ç–µ–∫–∞\"].astype(str).str.lower().isin(['–¥–∞', 'yes', 'true', '1', '–∏–ø–æ—Ç–µ–∫–∞'])]\n",
    "            current_banks = set(mort_deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].unique())\n",
    "            accumulated_banks.update(current_banks)\n",
    "            \n",
    "            # –°—á–∏—Ç–∞–µ–º —Å—É–º–º–∞—Ä–Ω—ã–π –≤–µ—Å –±–∞–Ω–∫–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ –ø—Ä–æ–µ–∫—Ç–µ –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç\n",
    "            curr_weight = sum([weights_map.get(b, 0) for b in accumulated_banks])\n",
    "            \n",
    "            # 2. –¢–ê–†–ì–ï–¢: –ü—Ä–æ–¥–∞–∂–∏ –¢–û–õ–¨–ö–û –≤ –º–µ—Å—è—Ü–µ m+1 (–ü—Ä–∏—Ä–æ—Å—Ç)\n",
    "            target_deals = p_deals[p_deals[\"Month_Num\"] == (m + 1)]\n",
    "            sales_sqm_in_next_month = target_deals[\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"].sum()\n",
    "            \n",
    "            # % –æ—Ç –æ–±—â–µ–π –ø–ª–æ—â–∞–¥–∏ (Increment Sellout %)\n",
    "            inc_sellout = (sales_sqm_in_next_month / total_area) * 100\n",
    "            \n",
    "            dataset.append({\n",
    "                \"Project\": project,\n",
    "                \"Month_Features\": m,          # –ú–µ—Å—è—Ü, –∫–æ–≥–¥–∞ –º—ã —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –±–∞–Ω–∫–∞—Ö\n",
    "                \"Month_Target\": m + 1,        # –ú–µ—Å—è—Ü, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º\n",
    "                \"Bank_Index_Accumulated\": curr_weight, # –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –≤–µ—Å –±–∞–Ω–∫–æ–≤\n",
    "                \"Banks_Count_Accumulated\": len(accumulated_banks),\n",
    "                \"Sellout_Increment\": inc_sellout # –°–∫–æ–ª—å–∫–æ % –ø–ª–æ—â–∞–¥–∏ –ø—Ä–æ–¥–∞–Ω–æ –≤ —Å–ª–µ–¥. –º–µ—Å—è—Ü–µ\n",
    "            })\n",
    "            \n",
    "    df_res = pd.DataFrame(dataset)\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä –≤—ã–±—Ä–æ—Å–æ–≤ (–µ—Å–ª–∏ –ø—Ä–æ–¥–∞–ª–∏ > 25% –¥–æ–º–∞ –∑–∞ –º–µ—Å—è—Ü - —ç—Ç–æ –æ–ø—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞, —É–±–∏—Ä–∞–µ–º –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã)\n",
    "    df_res = df_res[(df_res[\"Sellout_Increment\"] >= 0) & (df_res[\"Sellout_Increment\"] <= 25)]\n",
    "    \n",
    "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤. –°—Ç—Ä–æ–∫: {len(df_res)}\")\n",
    "    return df_res\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫\n",
    "df_model = create_incremental_dataset(proj_df, deals_df, bank_weights_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_and_diagnostics(df):\n",
    "    print(f\"\\nüöÄ –≠–¢–ê–ü 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (Prediction of Incremental Sales)\")\n",
    "    \n",
    "    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "    X = df[\"Bank_Index_Accumulated\"] # –ü—Ä–µ–¥–∏–∫—Ç–æ—Ä: –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –≤–µ—Å –±–∞–Ω–∫–æ–≤\n",
    "    y = df[\"Sellout_Increment\"]      # –¢–∞—Ä–≥–µ—Ç: –ü—Ä–æ–¥–∞–∂–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–µ—Å—è—Ü–∞\n",
    "    \n",
    "    X_const = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    \n",
    "    # --- –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"üìå R-squared: {model.rsquared:.4f} (–û–±—ä—è—Å–Ω—è—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å)\")\n",
    "    print(f\"üìå P-value (Bank Index): {model.pvalues['Bank_Index_Accumulated']:.4e}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Coef\": model.params,\n",
    "        \"P-val\": model.pvalues,\n",
    "        \"Significance\": model.pvalues.apply(lambda p: \"‚≠ê‚≠ê‚≠ê\" if p<0.001 else \"‚ùå\")\n",
    "    })\n",
    "    print(summary_df)\n",
    "    \n",
    "    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –ì–†–ê–§–ò–ö–û–í –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    plt.suptitle(\"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ú–æ–¥–µ–ª–∏ (Increment Prediction)\", fontsize=16)\n",
    "    \n",
    "    # 1. Scatter: –ë–∞–Ω–∫–∏ vs –ü—Ä–æ–¥–∞–∂–∏\n",
    "    sns.regplot(x=df[\"Bank_Index_Accumulated\"], y=df[\"Sellout_Increment\"], ax=axes[0], \n",
    "                scatter_kws={'alpha':0.1, 'color':'gray'}, line_kws={'color':'red'})\n",
    "    axes[0].set_title(\"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: –í–µ—Å –ë–∞–Ω–∫–æ–≤ vs –ü—Ä–∏—Ä–æ—Å—Ç –ü—Ä–æ–¥–∞–∂\")\n",
    "    axes[0].set_xlabel(\"–ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –≤–µ—Å –±–∞–Ω–∫–æ–≤\")\n",
    "    axes[0].set_ylabel(\"% –ü—Ä–æ–¥–∞–∂ –≤ —Å–ª–µ–¥. –º–µ—Å—è—Ü–µ\")\n",
    "    \n",
    "    # 2. –û—Å—Ç–∞—Ç–∫–∏\n",
    "    sns.histplot(model.resid, ax=axes[1], kde=True, color='purple')\n",
    "    axes[1].set_title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ (Residuals)\")\n",
    "    axes[1].set_xlabel(\"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞\")\n",
    "    \n",
    "    # 3. QQ Plot\n",
    "    sm.qqplot(model.resid, line='45', fit=True, ax=axes[2])\n",
    "    axes[2].set_title(\"Q-Q Plot (–ù–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_diagnostics.png\", dpi=100) # –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫\n",
    "model_result = run_model_and_diagnostics(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ccd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_incremental_dynamics(df):\n",
    "    print(f\"\\nüöÄ –≠–¢–ê–ü 4: –ê–Ω–∞–ª–∏–∑ –î–∏–Ω–∞–º–∏–∫–∏ –ü—Ä–∏—Ä–æ—Å—Ç–∞ (Sales Pace Analysis)\")\n",
    "    \n",
    "    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –º–µ—Å—è—Ü–∞–º –∂–∏–∑–Ω–∏ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "    monthly_stats = df.groupby(\"Month_Target\").agg({\n",
    "        \"Sellout_Increment\": \"mean\",\n",
    "        \"Bank_Index_Accumulated\": \"mean\"\n",
    "    })\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ —Å –¥–≤—É–º—è –æ—Å—è–º–∏\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # –û—Å—å 1 (–ë–∞—Ä—ã): –°–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–∞–ª–∏ –≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ (–ü—Ä–∏—Ä–æ—Å—Ç)\n",
    "    bars = ax1.bar(monthly_stats.index, monthly_stats[\"Sellout_Increment\"], \n",
    "                   color='skyblue', alpha=0.7, label='–°—Ä–µ–¥–Ω–∏–π % –ø—Ä–æ–¥–∞–∂ –∑–∞ –º–µ—Å—è—Ü (Increment)')\n",
    "    ax1.set_xlabel('–ù–æ–º–µ—Ä –º–µ—Å—è—Ü–∞ –∂–∏–∑–Ω–∏ –ø—Ä–æ–µ–∫—Ç–∞ (0 = –°—Ç–∞—Ä—Ç –ø—Ä–æ–¥–∞–∂)', fontsize=12)\n",
    "    ax1.set_ylabel('–°–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂ (% –æ—Ç –ø–ª–æ—â–∞–¥–∏ –≤ –º–µ—Å—è—Ü)', fontsize=12, color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_xticks(monthly_stats.index)\n",
    "    \n",
    "    # –û—Å—å 2 (–õ–∏–Ω–∏—è): –ö–∞–∫ —Ä–æ—Å –≤–µ—Å –±–∞–Ω–∫–æ–≤\n",
    "    ax2 = ax1.twinx()\n",
    "    line = ax2.plot(monthly_stats.index, monthly_stats[\"Bank_Index_Accumulated\"], \n",
    "             color='red', linewidth=3, marker='o', label='–ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –í–µ—Å –ë–∞–Ω–∫–æ–≤ (Index)')\n",
    "    ax2.set_ylabel('–°—Ä–µ–¥–Ω–∏–π –ò–Ω–¥–µ–∫—Å –ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è –ë–∞–Ω–∫–æ–≤', fontsize=12, color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # –õ–µ–≥–µ–Ω–¥–∞\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper center', fontsize=11)\n",
    "    \n",
    "    plt.title(\"–í–ª–∏—è–Ω–∏–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –±–∞–Ω–∫–æ–≤ –Ω–∞ –µ–∂–µ–º–µ—Å—è—á–Ω—ã–π —Ç–µ–º–ø –ø—Ä–æ–¥–∞–∂\", fontsize=16)\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–¥ –±–∞—Ä–∞–º–∏\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.savefig(\"incremental_sales_dynamics.png\", dpi=120)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- –¢–ï–ö–°–¢–û–í–û–ï –û–ü–ò–°–ê–ù–ò–ï (–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø) ---\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìù –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ê–Ø –°–ü–†–ê–í–ö–ê –ü–û –ì–†–ê–§–ò–ö–£\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"–ì—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é ¬´—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–æ –±–æ–ª—å–Ω–∏—Ü–µ¬ª –∑–∞ –ø–µ—Ä–≤—ã–µ 12 –º–µ—Å—è—Ü–µ–≤ –∂–∏–∑–Ω–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤.\")\n",
    "    print(\"\\n1. –°–ò–ù–ò–ï –°–¢–û–õ–ë–¶–´ (–¢–µ–º–ø –ø—Ä–æ–¥–∞–∂):\")\n",
    "    print(\"   –ü–æ–∫–∞–∑—ã–≤–∞—é—Ç, —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –æ—Ç –≤—Å–µ–≥–æ –¥–æ–º–∞ –ø—Ä–æ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –º–µ—Å—è—Ü.\")\n",
    "    print(\"   - –í—ã—Å–æ–∫–∏–π —Å—Ç–∞—Ä—Ç (–ú–µ—Å 1-2): –≠—Ñ—Ñ–µ–∫—Ç –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–≥–æ —Å–ø—Ä–æ—Å–∞ –∏ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —Ü–µ–Ω.\")\n",
    "    print(\"   - –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è (–ú–µ—Å 4-8): –í—ã—Ö–æ–¥ –Ω–∞ '–∫—Ä–µ–π—Å–µ—Ä—Å–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å'.\")\n",
    "    print(\"\\n2. –ö–†–ê–°–ù–ê–Ø –õ–ò–ù–ò–Ø (–ë–∞–Ω–∫–∏):\")\n",
    "    print(\"   –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –±—ã—Å—Ç—Ä–æ –ø—Ä–æ–µ–∫—Ç—ã –æ–±—Ä–∞—Å—Ç–∞—é—Ç –±–∞–Ω–∫–æ–≤—Å–∫–∏–º–∏ –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏—è–º–∏.\")\n",
    "    print(\"   - –†–µ–∑–∫–∏–π —Ä–æ—Å—Ç –≤ –Ω–∞—á–∞–ª–µ: –ó–∞—Å—Ç—Ä–æ–π—â–∏–∫–∏ —Å—Ç–∞—Ä–∞—é—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å –∫–ª—é—á–µ–≤—ã—Ö –∏–≥—Ä–æ–∫–æ–≤ —Å—Ä–∞–∑—É.\")\n",
    "    print(\"   - –ü–ª–∞—Ç–æ: –ü–æ—Å–ª–µ 6-8 –º–µ—Å—è—Ü–∞ –Ω–∞–±–æ—Ä –±–∞–Ω–∫–æ–≤ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è, –æ—Å—Ç–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∏—à–µ–≤—ã–µ –∏–≥—Ä–æ–∫–∏.\")\n",
    "    print(\"\\n3. –í–´–í–û–î –î–õ–Ø –ë–ò–ó–ù–ï–°–ê:\")\n",
    "    corr = df[[\"Sellout_Increment\", \"Bank_Index_Accumulated\"]].corr().iloc[0,1]\n",
    "    print(f\"   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–π –±–∞–∑–æ–π –±–∞–Ω–∫–æ–≤ –∏ –ø—Ä–æ–¥–∞–∂–∞–º–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–µ—Å—è—Ü–∞: {corr:.2f}\")\n",
    "    if corr > 0.1:\n",
    "        print(\"   ‚úÖ –ì–∏–ø–æ—Ç–µ–∑–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç: –ß–µ–º –º–æ—â–Ω–µ–µ –ø—É–ª –±–∞–Ω–∫–æ–≤ –∫ –Ω–∞—á–∞–ª—É –º–µ—Å—è—Ü–∞, —Ç–µ–º –≤—ã—à–µ –ø—Ä–æ–¥–∞–∂–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º.\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è –°–≤—è–∑—å —Å–ª–∞–±–∞—è: –í–æ–∑–º–æ–∂–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç '–Ω–æ–≤–∏–∑–Ω—ã' –ø—Ä–æ–µ–∫—Ç–∞, –∞ –Ω–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–Ω–∫–æ–≤.\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫\n",
    "plot_incremental_dynamics(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481760cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 6)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. –ì–ï–ù–ï–†–ê–¢–û–† –î–ê–¢–ê–°–ï–¢–ê (ETL)\n",
    "# ==============================================================================\n",
    "def prepare_incremental_dataset(\n",
    "    path_proj, \n",
    "    path_deals, \n",
    "    horizon_months=24,\n",
    "    bank_weight_metric='log_area' # 'log_area' –∏–ª–∏ 'count'\n",
    "):\n",
    "    \"\"\"\n",
    "    –°–æ–±–∏—Ä–∞–µ—Ç –ø–∞–Ω–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: –ñ–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–æ–µ–∫—Ç–∞ (Month 0 -> Horizon).\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ –ó–ê–ü–£–°–ö ETL: –°–±–æ—Ä–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ì–æ—Ä–∏–∑–æ–Ω—Ç: {horizon_months} –º–µ—Å)...\")\n",
    "    \n",
    "    # --- 1.1 –ú–∞–∫—Ä–æ-–¥–∞–Ω–Ω—ã–µ (–í—à–∏—Ç–∞—è –∏—Å—Ç–æ—Ä–∏—è —Å—Ç–∞–≤–∫–∏) ---\n",
    "    key_rate_data = [\n",
    "        ('2013-09-13', '2014-03-02', 5.50), ('2014-03-03', '2014-04-24', 7.00), ('2014-04-25', '2014-07-27', 7.50),\n",
    "        ('2014-07-28', '2014-11-04', 8.00), ('2014-11-05', '2014-12-11', 9.50), ('2014-12-12', '2014-12-15', 10.50),\n",
    "        ('2014-12-16', '2015-02-01', 17.00), ('2015-02-02', '2015-03-15', 15.00), ('2015-03-16', '2015-05-04', 14.00),\n",
    "        ('2015-05-05', '2015-06-15', 12.50), ('2015-06-16', '2015-08-02', 11.50), ('2015-08-03', '2016-06-13', 11.00),\n",
    "        ('2016-06-14', '2016-09-18', 10.50), ('2016-09-19', '2017-03-26', 10.00), ('2017-03-27', '2017-05-01', 9.75),\n",
    "        ('2017-05-02', '2017-06-18', 9.25), ('2017-06-19', '2017-09-17', 9.00), ('2017-09-18', '2017-10-29', 8.50),\n",
    "        ('2017-10-30', '2017-12-17', 8.25), ('2017-12-18', '2018-02-11', 7.75), ('2018-02-12', '2018-03-25', 7.50),\n",
    "        ('2018-03-26', '2018-09-16', 7.25), ('2018-09-17', '2018-12-16', 7.50), ('2018-12-17', '2019-06-16', 7.75),\n",
    "        ('2019-06-17', '2019-07-28', 7.50), ('2019-07-29', '2019-09-08', 7.25), ('2019-09-09', '2019-10-27', 7.00),\n",
    "        ('2019-10-28', '2019-12-15', 6.50), ('2019-12-16', '2020-02-09', 6.25), ('2020-02-10', '2020-04-26', 6.00),\n",
    "        ('2020-04-27', '2020-06-21', 5.50), ('2020-06-22', '2020-07-26', 4.50), ('2020-07-27', '2021-03-21', 4.25),\n",
    "        ('2021-03-22', '2021-04-25', 4.50), ('2021-04-26', '2021-06-14', 5.00), ('2021-06-15', '2021-07-25', 5.50),\n",
    "        ('2021-07-26', '2021-09-12', 6.50), ('2021-09-13', '2021-10-24', 6.75), ('2021-10-25', '2021-12-19', 7.50),\n",
    "        ('2021-12-20', '2022-02-13', 8.50), ('2022-02-14', '2022-02-27', 9.50), ('2022-02-28', '2022-04-10', 20.00),\n",
    "        ('2022-04-11', '2022-05-03', 17.00), ('2022-05-04', '2022-05-26', 14.00), ('2022-05-27', '2022-06-13', 11.00),\n",
    "        ('2022-06-14', '2022-07-24', 9.50), ('2022-07-25', '2022-09-18', 8.00), ('2022-09-19', '2022-12-31', 7.50),\n",
    "        ('2023-01-01', '2023-07-26', 7.5), ('2023-07-27', '2023-08-14', 8.5), ('2023-08-15', '2023-09-17', 12.0),\n",
    "        ('2023-09-18', '2023-10-29', 13.0), ('2023-10-30', '2023-12-17', 15.0), ('2023-12-18', '2024-07-28', 16.0),\n",
    "        ('2024-07-29', '2024-09-15', 18.0), ('2024-09-16', '2024-12-27', 19.0), ('2024-12-28', '2025-06-08', 21.0)\n",
    "    ]\n",
    "    macro_idx = pd.date_range(start='2018-01-01', end='2025-12-31', freq='D')\n",
    "    macro_df = pd.DataFrame(index=macro_idx)\n",
    "    macro_df['key_rate'] = np.nan\n",
    "    for start, end, rate in key_rate_data:\n",
    "        mask = (macro_df.index >= pd.to_datetime(start)) & (macro_df.index <= pd.to_datetime(end))\n",
    "        macro_df.loc[mask, 'key_rate'] = rate\n",
    "    macro_df = macro_df.ffill()\n",
    "\n",
    "    def get_avg_key_rate(dt_target):\n",
    "        try:\n",
    "            start_dt = dt_target.replace(day=1)\n",
    "            end_dt = start_dt + pd.offsets.MonthEnd(0)\n",
    "            subset = macro_df.loc[start_dt:end_dt]\n",
    "            return subset['key_rate'].mean() if not subset.empty else 10.0\n",
    "        except: return 10.0\n",
    "\n",
    "    # --- 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ ---\n",
    "    def clean_num(x):\n",
    "        if pd.isna(x): return 0\n",
    "        val = str(x).replace(\"\\u00a0\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "        return pd.to_numeric(val, errors=\"coerce\") or 0\n",
    "\n",
    "    proj = pd.read_excel(path_proj)\n",
    "    deals = pd.read_excel(path_deals)\n",
    "\n",
    "    for col in [\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\", \"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"]:\n",
    "        if col in proj.columns: proj[col] = proj[col].apply(clean_num)\n",
    "        if col in deals.columns: deals[col] = deals[col].apply(clean_num)\n",
    "\n",
    "    deals[\"dt_deal\"] = pd.to_datetime(deals[\"–î–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ (–º–µ—Å—è—Ü.–≥–æ–¥)\"], dayfirst=True, errors=\"coerce\")\n",
    "    deals = deals.dropna(subset=[\"dt_deal\"])\n",
    "    \n",
    "    # –ú—ç–ø–ø–∏–Ω–≥\n",
    "    for df in [proj, deals]:\n",
    "        df['ID –∫–æ—Ä–ø—É—Å–∞'] = df['ID –∫–æ—Ä–ø—É—Å–∞'].astype(str).str.replace(r'\\.0$', '', regex=True).str.strip()\n",
    "    deals = deals.merge(proj[[\"ID –∫–æ—Ä–ø—É—Å–∞\", \"–ü—Ä–æ–µ–∫—Ç\"]].drop_duplicates(), on=\"ID –∫–æ—Ä–ø—É—Å–∞\", how=\"left\").dropna(subset=[\"–ü—Ä–æ–µ–∫—Ç\"])\n",
    "\n",
    "    # --- 1.3 –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –±–∞–Ω–∫–æ–≤ ---\n",
    "    deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"] = deals[\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].fillna(\"–ù–µ –∏–ø–æ—Ç–µ–∫–∞\").astype(str).str.strip()\n",
    "    bank_stats = deals.groupby(\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\").agg({\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\": \"sum\", \"–ü—Ä–æ–µ–∫—Ç\": \"count\"}).reset_index()\n",
    "    \n",
    "    if bank_weight_metric == 'log_area':\n",
    "        bank_stats[\"weight\"] = np.log1p(bank_stats[\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"])\n",
    "    else:\n",
    "        bank_stats[\"weight\"] = np.log1p(bank_stats[\"–ü—Ä–æ–µ–∫—Ç\"]) # –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–¥–µ–ª–æ–∫\n",
    "        \n",
    "    weights_map = bank_stats.set_index(\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\")[\"weight\"].to_dict()\n",
    "\n",
    "    # --- 1.4 –°–±–æ—Ä–∫–∞ Dataset ---\n",
    "    proj_starts = deals.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[\"dt_deal\"].min().reset_index().rename(columns={\"dt_deal\": \"start_date\"})\n",
    "    proj_areas = proj.groupby(\"–ü—Ä–æ–µ–∫—Ç\")[\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"].sum().reset_index()\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: pre-calculate month offsets\n",
    "    deals = deals.merge(proj_starts, on=\"–ü—Ä–æ–µ–∫—Ç\", how=\"left\")\n",
    "    deals[\"Month_Offset\"] = ((deals[\"dt_deal\"].dt.year - deals[\"start_date\"].dt.year) * 12 + \\\n",
    "                             (deals[\"dt_deal\"].dt.month - deals[\"start_date\"].dt.month)).fillna(-1).astype(int)\n",
    "    \n",
    "    dataset = []\n",
    "    mortgage_flags = ['–¥–∞', 'yes', 'true', '1', '–∏–ø–æ—Ç–µ–∫–∞']\n",
    "    valid_projects = proj_areas[proj_areas[\"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"] > 100][\"–ü—Ä–æ–µ–∫—Ç\"].unique()\n",
    "\n",
    "    for project in valid_projects:\n",
    "        p_area = proj_areas.loc[proj_areas[\"–ü—Ä–æ–µ–∫—Ç\"] == project, \"–û–±—â–∞—è –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å\"].values[0]\n",
    "        p_start = proj_starts.loc[proj_starts[\"–ü—Ä–æ–µ–∫—Ç\"] == project, \"start_date\"].values[0]\n",
    "        \n",
    "        # –ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ —Å–¥–µ–ª–æ–∫ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "        p_deals = deals[deals[\"–ü—Ä–æ–µ–∫—Ç\"] == project]\n",
    "        accumulated_banks = set()\n",
    "        \n",
    "        for m in range(horizon_months):\n",
    "            # –°–æ—Å—Ç–æ—è–Ω–∏–µ (Features) –Ω–∞ –∫–æ–Ω–µ—Ü –º–µ—Å—è—Ü–∞ m\n",
    "            past_deals = p_deals[p_deals[\"Month_Offset\"] <= m]\n",
    "            \n",
    "            # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–Ω–∫–∏\n",
    "            new_banks = past_deals[past_deals[\"–ò–ø–æ—Ç–µ–∫–∞\"].astype(str).str.lower().isin(mortgage_flags)][\"–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–Ω–∫–∞\"].unique()\n",
    "            accumulated_banks.update(new_banks)\n",
    "            \n",
    "            curr_weight = sum([weights_map.get(b, 0) for b in accumulated_banks])\n",
    "            \n",
    "            # –ü—Ä–æ–¥–∞–∂–∏ –≤ m (–¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏)\n",
    "            curr_sales = past_deals[past_deals[\"Month_Offset\"] == m][\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"].sum()\n",
    "            \n",
    "            # –¶–µ–ª—å (Target) –≤ –º–µ—Å—è—Ü–µ m+1\n",
    "            target_m = m + 1\n",
    "            target_sales = p_deals[p_deals[\"Month_Offset\"] == target_m][\"–°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å —Å–¥–µ–ª–æ–∫\"].sum()\n",
    "            target_date = p_start + pd.DateOffset(months=target_m)\n",
    "            \n",
    "            # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "            so_inc = (target_sales / p_area) * 100\n",
    "            prev_so = (curr_sales / p_area) * 100\n",
    "            \n",
    "            dataset.append({\n",
    "                \"Project\": project,\n",
    "                \"Month_Num\": target_m,\n",
    "                \"Log_Total_Area\": np.log1p(p_area),\n",
    "                \"Key_Rate\": get_avg_key_rate(target_date),\n",
    "                \"Bank_Accumulated_Index\": curr_weight,\n",
    "                \"Banks_Count\": len(accumulated_banks),\n",
    "                \"Prev_Month_Sellout\": prev_so, # Lagged feature\n",
    "                \"Sellout_Increment\": so_inc\n",
    "            })\n",
    "\n",
    "    df_res = pd.DataFrame(dataset)\n",
    "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —è–≤–Ω—ã—Ö –æ—à–∏–±–æ–∫ (–≤–æ–∑–≤—Ä–∞—Ç—ã –∏ –æ–ø—Ç–æ–≤—ã–µ –≤—ã–∫—É–ø—ã > 30% –¥–æ–º–∞ –∑–∞ –º–µ—Å—è—Ü)\n",
    "    df_res = df_res[(df_res[\"Sellout_Increment\"] >= 0) & (df_res[\"Sellout_Increment\"] < 30)]\n",
    "    \n",
    "    print(f\"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–±—Ä–∞–Ω: {len(df_res)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π.\")\n",
    "    return df_res\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. –ú–û–î–ï–õ–¨ –ò –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê (–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ê–Ø –§–£–ù–ö–¶–ò–Ø)\n",
    "# ==============================================================================\n",
    "def train_incremental_model(\n",
    "    df, \n",
    "    features, \n",
    "    target=\"Sellout_Increment\", \n",
    "    title=\"–ú–æ–¥–µ–ª—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂\"\n",
    "):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–µ—Ç OLS –º–æ–¥–µ–ª—å –Ω–∞ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö —Ñ–∏—á–∞—Ö –∏ –≤—ã–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç.\n",
    "    \"\"\"\n",
    "    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n",
    "    data = df.dropna(subset=features + [target]).copy()\n",
    "    X = sm.add_constant(data[features])\n",
    "    y = data[target]\n",
    "    \n",
    "    # 2. –û–±—É—á–µ–Ω–∏–µ\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    preds = model.predict(X)\n",
    "    \n",
    "    # 3. –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    r2_adj = model.rsquared_adj\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y, preds))\n",
    "    mean_y = y.mean()\n",
    "    \n",
    "    # 4. –í—ã–≤–æ–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"üî¨ –ú–û–î–ï–õ–¨: {title}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    \n",
    "    # –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"–ú–µ—Ç—Ä–∏–∫–∞\": [\"Adj. R-squared\", \"RMSE (–û—à–∏–±–∫–∞ % –ø–ª–æ—â–∞–¥–∏)\", \"MAE (–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ–º–∞—Ö)\", \"Mean Target (–°—Ä–µ–¥–Ω–∏–π —Ç–µ–º–ø)\"],\n",
    "        \"–ó–Ω–∞—á–µ–Ω–∏–µ\": [r2_adj, rmse, mae, mean_y],\n",
    "        \"–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π\": [\n",
    "            \"–ß–µ–º –±–ª–∏–∂–µ –∫ 1.0, —Ç–µ–º –ª—É—á—à–µ\", \n",
    "            f\"–í —Å—Ä–µ–¥–Ω–µ–º –æ—à–∏–±–∞–µ–º—Å—è –Ω–∞ {rmse:.2f} –ø.–ø.\", \n",
    "            f\"–ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ {mae:.2f} –ø.–ø.\", \n",
    "            f\"–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂: {mean_y:.2f}% –≤ –º–µ—Å\"\n",
    "        ]\n",
    "    })\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # –¢–∞–±–ª–∏—Ü–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (Beta)\": model.params,\n",
    "        \"Std. Error\": model.bse,\n",
    "        \"P-value\": model.pvalues,\n",
    "        \"–ó–Ω–∞—á–∏–º–æ—Å—Ç—å\": model.pvalues.apply(lambda p: \"‚≠ê‚≠ê‚≠ê\" if p<0.001 else (\"‚≠ê‚≠ê\" if p<0.05 else (\"‚≠ê\" if p<0.1 else \"‚ùå\")))\n",
    "    })\n",
    "    \n",
    "    # –°–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã\n",
    "    trans_map = {\n",
    "        \"const\": \"–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (–ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å)\",\n",
    "        \"Bank_Accumulated_Index\": \"üè¶ –ò–Ω–¥–µ–∫—Å –í–µ—Å–∞ –ë–∞–Ω–∫–æ–≤\",\n",
    "        \"Banks_Count\": \"üî¢ –ö–æ–ª-–≤–æ –ë–∞–Ω–∫–æ–≤\",\n",
    "        \"Month_Num\": \"üìÖ –í–æ–∑—Ä–∞—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ (–º–µ—Å)\",\n",
    "        \"Key_Rate\": \"üìâ –ö–ª—é—á–µ–≤–∞—è –°—Ç–∞–≤–∫–∞ –¶–ë\",\n",
    "        \"Log_Total_Area\": \"üèó –ú–∞—Å—à—Ç–∞–± –ø—Ä–æ–µ–∫—Ç–∞ (Log)\",\n",
    "        \"Prev_Month_Sellout\": \"üîÑ –ü—Ä–æ–¥–∞–∂–∏ –ø—Ä–æ—à–ª–æ–≥–æ –º–µ—Å—è—Ü–∞ (Lag)\"\n",
    "    }\n",
    "    coef_df.index = coef_df.index.map(lambda x: trans_map.get(x, x))\n",
    "    print(coef_df)\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    # 5. –ì—Ä–∞—Ñ–∏–∫–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (3 —à—Ç)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    plt.suptitle(f\"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: {title}\", fontsize=14, weight='bold')\n",
    "\n",
    "    # –ê) –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã (–°–∏–ª–∞ –≤–ª–∏—è–Ω–∏—è)\n",
    "    std_params = model.params * (data[features + [target]].std() / data[target].std())\n",
    "    std_params = std_params.drop(\"const\", errors='ignore')\n",
    "    std_params.index = std_params.index.map(lambda x: trans_map.get(x, x))\n",
    "    std_params.sort_values().plot(kind='barh', ax=axes[0], color='#4c72b0')\n",
    "    axes[0].set_title(\"–°–∏–ª–∞ –≤–ª–∏—è–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (Std. Beta)\")\n",
    "    axes[0].axvline(0, color='black', linewidth=1)\n",
    "    \n",
    "    # –ë) –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑\n",
    "    axes[1].scatter(y, preds, alpha=0.3, color='purple', s=15)\n",
    "    axes[1].plot([0, y.max()], [0, y.max()], 'r--', lw=2)\n",
    "    axes[1].set_xlabel(\"–§–ê–ö–¢ (% –ø—Ä–æ–¥–∞–∂)\")\n",
    "    axes[1].set_ylabel(\"–ü–†–û–ì–ù–û–ó –º–æ–¥–µ–ª–∏\")\n",
    "    axes[1].set_title(f\"–¢–æ—á–Ω–æ—Å—Ç—å (R2 Adj: {r2_adj:.3f})\")\n",
    "    \n",
    "    # –í) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
    "    residuals = model.resid\n",
    "    sns.histplot(residuals, kde=True, ax=axes[2], color='green')\n",
    "    axes[2].set_title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ (Residuals)\")\n",
    "    axes[2].axvline(0, color='red', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø (–ó–ê–ü–£–°–ö)\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ef3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 1: –û–¥–∏–Ω —Ä–∞–∑ –≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-20 —Å–µ–∫—É–Ω–¥)\n",
    "# –£–∫–∞–∂–∏—Ç–µ —Å–≤–æ–∏ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º\n",
    "FILE_PROJ = \"–ü—Ä–æ–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ_2025-11-25.xlsx\"\n",
    "FILE_DEALS = \"–°–¥–µ–ª–∫–∏_2025-11-25.xlsx\"\n",
    "\n",
    "\n",
    "df_inc = prepare_incremental_dataset(FILE_PROJ, FILE_DEALS, horizon_months=12)\n",
    "\n",
    "# –®–ê–ì 2: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–µ–º —Å –º–æ–¥–µ–ª—è–º–∏\n",
    "\n",
    "# --- –í–ê–†–ò–ê–ù–¢ –ê: \"–ß–∏—Å—Ç—ã–π\" —ç—Ñ—Ñ–µ–∫—Ç –±–∞–Ω–∫–æ–≤ (–±–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏) ---\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—É: –í–ª–∏—è—é—Ç –ª–∏ –±–∞–Ω–∫–∏ –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏, –µ—Å–ª–∏ —É—á–µ—Å—Ç—å —Å—Ç–∞–≤–∫—É –∏ –≤–æ–∑—Ä–∞—Å—Ç?\n",
    "train_incremental_model(\n",
    "    df_inc, \n",
    "    features=['Bank_Accumulated_Index'], \n",
    "    title=\"–í–∞—Ä–∏–∞–Ω—Ç –ê: –í–ª–∏—è–Ω–∏–µ –¢–û–õ–¨–ö–û –±–∞–Ω–∫–æ–≤ \"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daadd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- –í–ê–†–ò–ê–ù–¢ –ê: \"–ß–∏—Å—Ç—ã–π\" —ç—Ñ—Ñ–µ–∫—Ç –±–∞–Ω–∫–æ–≤ (–±–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏) ---\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—É: –í–ª–∏—è—é—Ç –ª–∏ –±–∞–Ω–∫–∏ –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏, –µ—Å–ª–∏ —É—á–µ—Å—Ç—å —Å—Ç–∞–≤–∫—É –∏ –≤–æ–∑—Ä–∞—Å—Ç?\n",
    "train_incremental_model(\n",
    "    df_inc, \n",
    "    features=['Bank_Accumulated_Index', 'Month_Num', 'Key_Rate', 'Log_Total_Area'], \n",
    "    title=\"–í–∞—Ä–∏–∞–Ω—Ç –ê: –í–ª–∏—è–Ω–∏–µ –±–∞–Ω–∫–æ–≤ + –ú–∞–∫—Ä–æ\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139577c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- –í–ê–†–ò–ê–ù–¢ –ë: \"–ü–æ–ª–Ω—ã–π —Ñ–∞—Ä—à\" (—Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π) ---\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–¥–∞–∂–∏ –ø—Ä–æ—à–ª–æ–≥–æ –º–µ—Å—è—Ü–∞. R2 –≤—ã—Ä–∞—Å—Ç–µ—Ç, –Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç \"—Å—ä–µ—Å—Ç—å\" –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –±–∞–Ω–∫–æ–≤.\n",
    "train_incremental_model(\n",
    "    df_inc, \n",
    "    features=['Bank_Accumulated_Index', 'Month_Num', 'Key_Rate', 'Log_Total_Area', 'Prev_Month_Sellout'], \n",
    "    title=\"–í–∞—Ä–∏–∞–Ω—Ç –ë: –° —É—á–µ—Ç–æ–º –∏–Ω–µ—Ä—Ü–∏–∏ –ø—Ä–æ–¥–∞–∂ (Lag)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inc = prepare_incremental_dataset(FILE_PROJ, FILE_DEALS, horizon_months=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95617256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å CatBoost, –µ—Å–ª–∏ –Ω–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º Random Forest\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    MODEL_TYPE = \"CatBoost\"\n",
    "except ImportError:\n",
    "    MODEL_TYPE = \"RandomForest\"\n",
    "\n",
    "def train_ml_model(df, target=\"Sellout_Increment\"):\n",
    "    print(f\"üöÄ –ó–ê–ü–£–°–ö ML ({MODEL_TYPE}): –ü—Ä–æ–±—É–µ–º –ø–æ–±–∏—Ç—å –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é...\")\n",
    "    'Bank_Accumulated_Index'\n",
    "    # 1. –í—ã–±–∏—Ä–∞–µ–º —Ñ–∏—á–∏\n",
    "    # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –∏ —Ç–∞—Ä–≥–µ—Ç. –û—Å—Ç–∞–≤–ª—è–µ–º –≤—Å—ë —á–∏—Å–ª–æ–≤–æ–µ.\n",
    "    # –ï—Å–ª–∏ –≤ df_inc –µ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (–ö–ª–∞—Å—Å, –†–µ–≥–∏–æ–Ω), –¥–ª—è RF –∏—Ö –Ω–∞–¥–æ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å, \n",
    "    # –∞ CatBoost —Å—ä–µ—Å—Ç —Ç–∞–∫. –ü–æ–∫–∞ –±–µ—Ä–µ–º —á–∏—Å–ª–æ–≤—ã–µ.\n",
    "    features = [c for c in df.columns if c not in [\"Project\", \"start_date\", 'Prev_Month_Sellout','Bank_Accumulated_Index', target]]\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏\n",
    "    data = df.dropna(subset=features + [target])\n",
    "    \n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    # 2. Split –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç (80% / 20%)\n",
    "    # shuffle=False –≤–∞–∂–Ω–æ, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ —É –Ω–∞—Å –ø–∞–Ω–µ–ª—å–∫–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤,\n",
    "    # –ª—É—á—à–µ –ø–µ—Ä–µ–º–µ—à–∞—Ç—å, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} —Å—Ç—Ä–æ–∫\")\n",
    "    print(f\"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:  {X_test.shape[0]} —Å—Ç—Ä–æ–∫\")\n",
    "    \n",
    "    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    if MODEL_TYPE == \"CatBoost\":\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=1000, \n",
    "            learning_rate=0.05, \n",
    "            depth=6, \n",
    "            verbose=100,\n",
    "            loss_function='RMSE'\n",
    "        )\n",
    "        # –ï—Å–ª–∏ –±—ã–ª–∏ –±—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏, –ø–µ—Ä–µ–¥–∞–ª–∏ –±—ã –ø–∞—Ä–∞–º–µ—Ç—Ä cat_features=[...]\n",
    "        model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "    else:\n",
    "        # Random Forest (–µ—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ç–±—É—Å—Ç–∞)\n",
    "        model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "    # 4. –ü—Ä–µ–¥–∏–∫—Ç\n",
    "    preds_test = model.predict(X_test)\n",
    "    preds_train = model.predict(X_train)\n",
    "    \n",
    "    return model, X_train, X_test, y_train, y_test, preds_train, preds_test, features\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º\n",
    "model_ml, X_train, X_test, y_train, y_test, preds_train, preds_test, feature_names = train_ml_model(df_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa84f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_results(y_true, y_pred, model, features, title=\"ML Model Results\"):\n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–ï:\")\n",
    "    print(f\"   R-squared: {r2:.4f} (–õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–≤–∞–ª–∞ –æ–∫–æ–ª–æ 0.2-0.4?)\")\n",
    "    print(f\"   RMSE: {rmse:.4f} % (–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç –ø–ª–æ—â–∞–¥–∏)\")\n",
    "    print(f\"   MAE:  {mae:.4f} %\")\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫–∏\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    plt.suptitle(f\"–ê–Ω–∞–ª–∏–∑ ML –º–æ–¥–µ–ª–∏ ({MODEL_TYPE})\", fontsize=16)\n",
    "    \n",
    "    # 1. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'get_feature_importance'):\n",
    "        importances = model.get_feature_importance()\n",
    "    else:\n",
    "        importances = [0]*len(features)\n",
    "        \n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã\n",
    "    fi_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "    fi_df = fi_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=fi_df, ax=axes[0], palette=\"viridis\")\n",
    "    axes[0].set_title(\"–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–µ–∞–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏?\")\n",
    "    axes[0].set_xlabel(\"–í–∞–∂–Ω–æ—Å—Ç—å (Feature Importance)\")\n",
    "    \n",
    "    # 2. –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑\n",
    "    axes[1].scatter(y_true, y_pred, alpha=0.4, color='dodgerblue', s=20)\n",
    "    \n",
    "    # –ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è\n",
    "    limits = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "    axes[1].plot(limits, limits, 'r--', lw=2, label='–ò–¥–µ–∞–ª')\n",
    "    \n",
    "    axes[1].set_xlabel(\"–†–µ–∞–ª—å–Ω—ã–π —Ñ–∞–∫—Ç (Increment Sellout %)\")\n",
    "    axes[1].set_ylabel(\"–ü—Ä–æ–≥–Ω–æ–∑ –º–æ–¥–µ–ª–∏\")\n",
    "    axes[1].set_title(f\"–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∞ (R2: {r2:.3f})\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_ml_results(y_test, preds_test, model_ml, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
